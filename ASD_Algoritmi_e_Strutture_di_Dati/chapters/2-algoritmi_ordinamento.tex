\chapter{Calcolo tempo di esecuzione algoritmi}
In questo capitolo vedremo come calcolare il tempo di esecuzione di algoritmi,
partiremo ad analizzare algoritmi di ordinamento, più che per la loro
funzione primaria, per studiare come è possibile affrontare un problema con diverse
tecniche e come l'utilizzo di diverse tecniche influenzi anche notevolmente l'efficienza.
\section{Selection Sort}
Ordinamento per selezione, dove per selezione intendo che ad ogni passo seleziono
il valore minimo presente nell'array, scambio l'elemento più piccolo con l'elemento in
prima posizione, mi sposto sul secondo valore e cerco il più piccolo, andrà a sostituirlo
nella seconda posizione, e così via fino a quando non ho un solo valore da ordinare.\\
Qui di seguito il codice:
\begin{lstlisting}[language=Java]
void SelSort(int A[])
  (n-1)+1  For i = 1 to length(A) - 1
  (n-1)       Pmin = i
  sum(n-i)         For j = i + 1 to length(A)
  sum(n-i)              if A[j] < A[Pmin]
  t-if                      Pmin = j
  //Variabile appoggio per lo scambio
  (n-1)            app = A[i] 
  (n-1)            A[i] = A[Pmin]
  (n-1)            A[Pmin] = app
\end{lstlisting}
Length(A) - 1, perchè l'ultimo valore sono sicuro che sarà il più grande di tutti
dato che per gli tutti gli altri elementi ho cercato il minimo.\\
\paragraph*{Tempo esecuzione} Il For viene eseguito (n-1)+1 volte perchè devo
considerare anche il controllo finale che viene effettuato.\\
Il secondo For invece è più complesso da gestire perchè dipende anche da i che è esterna
al ciclo stesso. Ogni volta che eseguo il secondo For l'array si restringe di 1 perchè ogni
volta ordino un valore (trovando il minimo) quindi avrò una progressione del tipo $(n + (n-1) +
(n-2) + (n-3) + \dots + (1))$, quindi $(n-i) + 1 $, devo considerare perchè che verrà eseguito
ogni volta che il primo For viene eseguito quindi $\sum_{i=1}^{n-1}(n-i)$.\\
Il tempo di esecuzione sarà quindi:
\begin{equation*}
    5(n-1) + 2\sum{i=1}^{n-1} (n-1) + t_{if}
\end{equation*}
Dove 5(n-1) e la sommatoria non dipendono dall'input, mentre $t_if$ sì.
\paragraph*{Ricerca e Analisi caso peggiore} Il caso peggiore è A ordinato al contrario,
dato che in questo caso l'if viene eseguito ogni volta (dato che A[j] sarà sempre minore di A[Pmin]).\\
$t_if$ avrà il seguente tempo di esecuzione $(n-1)+(n-2)+(n-3) + \dots + 1 = \sum_{i=1}^{n-1} = \sum_{i=1}^{n}i$\\
Quindi:
\begin{equation*}
    T_p(n) = 5(n-1) + 3\sum_{i=1}^{n-1}(n-1) = 5(n-1) + 3\sum_{i=1}^{n}i
\end{equation*}
Ricordiamo questa equivalenza:
\begin{equation*}
    \sum_{i=1}^{f}i = \frac{f(f+1)}{2}
\end{equation*}
Quindi sostituendo otteniamo:
\begin{equation*}
    5(n-1)+3\frac{(n-1)n}{2}
\end{equation*}
Dato che a noi interessa l'ordine di grandezza e non ci interessano i dettagli possiamo approssimare questo
risultato come:
\begin{equation*}
    \approx 5n + n^2 \approx n^2
\end{equation*}
\paragraph*{Nota} In realtà se è decrescente quando scambio il minimo in fondo all'array con il primo valore,
che sarà il maggiore, sta ordinando entrambi gli elementi, però per semplificazione consideriamo che
l'if viene eseguito ogni volta.
\paragraph*{Ricerca e Analisi caso migliore}
Il caso migliore è quando l'array è ordinato dato che non eseguo mai l'if, quindi dato che
$t_{if} = 0$ ottengo:
\begin{equation*}
    t_m(n) = 5(n-1) + 2 \sum_{i=1}^{n-1}(n-1)+0 = 5(n-1) + \frac{2}{2}(n-1)n = 5n +n^2\\
    \approx n^2
\end{equation*}
Notiamo quindi che il caso migliore e peggiore non sono molto diversi, anzi hanno lo stesso
ordine di grandezza.

\section*{Insertion Sort}
In questo caso ordino partendo dal primo numero e controllando il secondo, verifico se il primo
è maggiore del secondo e nel caso li scambio, poi controllo il terzo numero e procedo a ordinarlo
insieme a primi due e così via.
\paragraph*{Esempio} \'E come ordinare un mazzo di carte pescandole mano, quindi inizialmente ho due
carte, le ordino confrontandole ed eventualmente scambiandole, poi pesco la terza e la ordino
con le altre due e così via, fino a quando non ho pescato tutte le carte.
\subsection{Implementazione}
\begin{lstlisting}[language=Java]
    void InsSort(A[])
(n-1)c  For i = 2 to length(A)
(n-1)c      App = A[i]
(n-1)c      j = j - 1
sum(tw)     while App < A[j] AND j > 0
sum(tw)         A[j+1] = A[j]
sum(tw)         j--
(n-1)c      A[j+1] = App
\end{lstlisting}
Uso il For perchè in ogni caso devo ordinare tutti i numeri dell'array, parto da
2 perchè un numero da solo è ordinato, quindi è inutile analizzarlo, inoltre j sarebbe uguale a 0,
e noi partiamo a contare da 1 gli indici dell'array.\\
\subsection{Tempi d'esecuzione, caso migliore e peggiore}
\paragraph*{Tempo di esecuzione} Faremo alcune approssimazioni dato che cerchiamo l'ordine
di grandezza dei tempi di esecuzione:
\begin{itemize}
    \item Non consideriamo l'ultima istruzione di controllo del For (quella che ci fa uscire dal ciclo)
    \item Non considero neanche l'ultima istruzione di controllo del While
\end{itemize}
Il For quindi richiederà n-1 istruzioni, dove \textbf{c} è un generico tempo di esecuzione per ogni istruzione,
dato che parte da 2 e non da 1.\\
Il while non ha un numero di volte fisso per cui è vero, dipende dall'input! Ci sarà un caso
peggiore e uno migliore, scrivo quindi $\sum_{i=2}^n{t_w}$ per indicare il numero di volte in cui il While è vero, 
la sommatoria è inserita perchè devo sommare tutte le esecuzioni While per ogni i-esima esecuzione 
del ciclo For.\\
Qui di seguito la spiegazione della sommatoria
\begin{align*}
    tw_{i=2} + tw_{i=3} + tw_{i=4} + \dots + tw_{i=n} \\
    \sum_{i=2}^n{tw_i}
\end{align*}
Il tempo di esecuzione è quindi
\begin{equation*}
    t_{is}(n) = 4c(n-1)+3c \sum_{i=2}^n tw_i
\end{equation*}
Dato che non è possibile definire in maniera univoca il numero di volte in cui
il while verrà eseguito andiamo a determinare il caso migliore e quello peggiore.
\paragraph*{Caso migliore} Il vettore è già ordinato. Analizzando il codice dobbiamo
verificare cosa può cambiare, in questo caso il While, quindi qual è il caso dove
il while viene eseguito meno volte? Quando App $<$ A[j] sempre, quindi $tw_i = 0$ 
per ogni i e questo accade quando A[] è già ordinato.
\begin{equation*}
    t_m(n) = 4c(n-1)+3c*0=4c(n-1)
\end{equation*}
\paragraph*{Caso peggiore} Il while viene eseguito il maggior numero di volte per ogni
i, cioè $tw_i=j=i-1$. Questo si verifica quando A è ordinato al contrario.\\
\begin{equation*}
    T_p(n) = 4c(n-1)+3c \sum_{i=2}^n (i-1)
\end{equation*}
Converto la sommatoria
\begin{equation*}
    \sum_{i=2}^n i-1 = 1+2+3+\dots+(n-1) = \sum_{i=1}^{n-1}i = \frac{(n-1)n}{2}
\end{equation*}
Inserisco nella formula
\begin{equation*}
    4c(n-1)+3c \frac{(n-1)n}{2} \approx 4cn+3c \frac{n^2}{2} \approx n^2
\end{equation*}
\paragraph*{Approssimazione caso migliore} Il caso migliore lo posso approssimare come n.
\subsection{Confronto tra Selection e Insertion sort}
Grazie alle approssimazioni possiamo confrontare i due algoritmi e notiamo subito che
il caso migliore di Insertion è migliore di Selection Sort dato che è nel primo caso è n,
mentre nel secondo è $n^2$.\\
Insertion Sort è quindi migliore di Selection Sort.
\section{Nozioni per rappresentare i tempi di esecuzione}
\begin{equation*}
    T(n) = O(n^2)
\end{equation*}
O (o grande) indica il limite asintotico superiore del tempo di esecuzione (il caso peggiore),
in altre parole indica il tempo massimo che posso aspettare per ricevere il risultato.\\
Mentre:
\begin{equation*}
    t(n) = \Omega(n)
\end{equation*}
Indica il limite inferiore del tempo di esecuzione (il caso migliore), quindi il tempo
minimo che devo aspettare.\\
Nei casi dove questi due casi corrispondono si indicano con:
\begin{equation*}
    \Theta(n^2)
\end{equation*}
$\Theta$ indica che il tempo è uguale (approssimativamente) per tutti gli input.
\subsection{Limiti asintotici}
I limiti asintotici sono delle funzioni per cui sono sicuro che la funzione presa in
considerazione $f(n)$ è sempre minore del limiti asintotico. In questo caso
dico che la funzione è asintoticamente limitata $f(n) = O(n^2)$.
%Riprendere spiegazione teoria dato che è una possibile domanda d'esame
Le lettere O, $\Omega$, $\Theta$, indicano una costante moltiplicativa per cui posso
moltiplicare la funzione e trova una funziona per la quale la mia funzione n non sarà
mai maggiore (nel caso di O), o minore (nel caso di $\Omega$) della funzione che la limita
asintoticamente. Questa costante indica tutto ciò che ho trascurato nel calcolo dei tempi, 
posso sceglierla a piacere per verificare che la funzione non violerà mia i limiti
asintotici. (FORMALIZZARE QUESTA PARTE AGGIUNGENDO FORMULE - GUARDA APPUNTI IPAD)
\begin{align*}
    O(g(n)) = \{f(n):\exists k>0, n_0 \geq 0 \,\, \text{t.c}\,\, \\
    0 \leq f(n) \leq k*g(n), \forall n \geq n_0 \}
\end{align*}
Questo ci porterà a fare delle approssimazioni come per esempio non considerare
l'ultimo controllo del for o del while, questo produrrà delle situazioni che possono
sembrare non sensate, per esempio potrà risultare che la condizione del while non viene mai
eseguita, in realtà non è vero, perchè almeno 1 volta verrà eseguito, però per le approssimazioni
che abbiamo scelto di fare non lo consideriamo.
\paragraph*{Attenzione}
\begin{enumerate}
    \item Devo scegliere la funzione che solo per alcune costanti sia più grande/piccola di quella confrontata
    ($\exists k \,\, \text(t.c)$),
    altrimenti sto scegliendo un o (o-piccolo) dove la funzione è grande/piccola per ogni costante scelta
    ($\forall$).
    \item Non mi interessa se il il termine di ordine maggiore sia seguito da addizioni o sottrazioni
    \item Di fronte a un tempo costante come $\Omega$ indichiamo 1
\end{enumerate}
\subsection{Proprietà}
O, $\Omega$, $Theta$ hanno le seguenti proprietà:
\begin{itemize}
    \item Sono Transitivi
    \item Simmetria vale solo per $\Theta$
    \item Simmetria trasposta per O e $\Omega$ - Es. $2n = O(n^2) \,\, n^2=\Omega(2n)$
\end{itemize}

\section{Esercizio ricerca elementi V2 in V1}
\paragraph*{Richiesta} Dati 2 vettori V1 e V2, con n valori interni, quanti elementi di
V2 compaiono in V1? Valutare quindi i tempi di esecuzione dell'algoritmo.
\paragraph*{Osservazioni} Dati i seguenti due vettori
\begin{center}
    V1 = (7, 4, 4, 4, 12)\\
    V2 = (3, 7, 4, 15, 20)
\end{center}
Gli elementi di V2 che compaiono in V1 sono 2, il 4 non devo contarlo più volte!\\
Viceversa, se ci fosse stato il 4 più volte in V2 e anche 1 sola volta in V1, andava contato
più volte.
\subsection{Implementazione}
\begin{lstlisting}[language=Java]
    int confronta(V1[], V2[])
c*1        cont = 0
c*n        for i=0 to length(V2)
c*n            j=1
sum(c*twi)     while (V2[i] != V1[j]) AND j <= length(V1)
sum(c*twi)        j++;
c*n            if j <= length(V1)
c*tif             cont++
c*1        Return(cont)
\end{lstlisting}
Dove sum(c*twi) sarebbe $\sum_{i=1}^n{c*tw_i}$\\
\paragraph*{Tempo algoritmo}
\begin{equation*}
    2c*1 + 3c*n + c*t_if + 2c \sum_{i=1}^n{tw_i}
\end{equation*}
Posso portare fuori la c e il 2 dalla sommatoria dato che sono 2 costanti.
\paragraph*{Ricerca caso peggiore}
Analizzo le due istruzioni variabili, cioè
\begin{itemize}
    \item if
    \item while
\end{itemize}
Quale pesa di più? Il controllo dell'if viene eseguito ogni volta, l'incremento del conteggio non sempre, però conta comunque
1 istruzione, e comunque l'if viene sempre eseguito, mentre il while può essere rieseguito
diverse volte, questo mi suggerisce che devo cercare un caso dove
il while viene eseguito molte volte, dato che è potenzialmente molto più pesante dell'if.\\
In generale ci capiteranno sempre 2 quantità in contrasto, come in questo caso abbiamo l'if e il while, dovremo
determinare la quantità più pesante.\\
Il caso peggiore è quindi quello in cui il while cicla il maggior numero di volte possibile, quando $tw_i$ è max per
ogni i, quindi quando nessun elemento di V2 è presente in V1.\\
\paragraph*{Tempo}
\begin{align*}
    tw_1=n, \,\, tw_2=n \,\, \dots tw_n=n\\
    T_p(n) = 2c+3c*n+c*t_if+2c\sum_{i=1}^n{n}\\
    =2c+3c*n+0+2cn^2
\end{align*}
Perchè $\sum_{i=1}^n{n}=n+n+n+n+n ... \,\, \text{per n volte } = n^2$\\
Da questo risultato mi rendo conto che l'ordine di grandezza è $n^2$, per esprimerlo
in maniera formale scrivo:
\begin{align*}
    = O(n^2)
\end{align*}
\paragraph*{Ricerca caso migliore}
Il caso migliore è quello dove il while non viene mai eseguito (0 esecuzioni del while
ad ogni iterazione del for).\\
Il caso migliore NON è i 2 array sono uguali, perchè ci saranno delle iterazioni in cui il while verrà
eseguito, il caso migliore è quando V2 contiene un solo elemento ripetuto n volte e lo stesso
elemento è presente in V1[1].
\begin{center}
    V1 1,2,3,4\\
    V2 1,1,1,1
\end{center}
\paragraph*{Tempo}
\begin{align*}
    tw_i = 0 \quad \forall i \rightarrow t_if = n\\
    t_m(n) = 2c + 3cn + cn + 2c \sum_{i=1}^n {0}\\
    =\Omega(n)
\end{align*}
In questo caso (opposto al precedente) while non viene mai eseguito e if n volte.\\
\paragraph*{Verifica caso medio}
\'E ragionevole pensare che su molte esecuzioni il primo valore di V2 in V1 si trovi
mediamente al centro.\\
T medio$(n) \rightarrow V_2[j]$ viene trovato sempre in $V_1(\frac{n}{2}) \rightarrow tw_i = \frac{n}{2}\forall i$\\
\begin{align*}
    =2c+3cn+cn+2c\sum_{i=1}^n{frac{n}{2}} = \\
    =2c+4cn+cn^2 = \Theta(n^2)
\end{align*}
\paragraph*{Riassumendo}
\begin{itemize}
    \item Tempo peggiore - $O(n^2)$
    \item Tempo migliore - $\Omega(n)$
    \item Tempo medio - $\Theta(n^2)$
\end{itemize}
\section{Riassunto ordini di grandezza}
Qui di seguito sono riportati gli ordini di grandezza in ordine crescente 
\begin{itemize}
    \item 1 (intesa come costante)
    \item $\log n$
    \item $n$
    \item $n\log n$
    \item $n^2$
    \item $n^3$
    \item $2^n$
\end{itemize}

\section{Somma di due valori costituiti da bit}
\paragraph*{Testo} Sommare due valori binari contenuti in due array e salvare il risultato
in un terzo array.
\paragraph*{Struttura}
\begin{center}
    A[1 \dots n]\\
    B[1 \dots n]\\
    C[1 \dots n+1]
\end{center}
Dove i bit più significativi sono quelli a sinistra (quelli iniziali) e quelli meno
significativi a destra.
\paragraph*{Considerazioni} Devo effettuare la somma bit a bit, quindi si configurano 3 casi
\begin{itemize}
    \item Sommo 0 con 0 e ottengo 0
    \item Sommo 1 con 0 (o 0 con 1) e ottengo 1
    \item Sommo 1 con 1 e ottengo 1, ma devo ricordarmi del riporto!
\end{itemize}
Conviene quindi gestire ogni somma come una somma a 3 bit (bit A, bit B, bit di riporto).
\subsection{Implementazione}
\begin{lstlisting}[language=Java]
    void Somma(A[],B[])
    c*1 riporto = 0;
        //parola chiave per indicare i-- nel for e' Downto
    c*n for i = length(A) Downto 1 
    c*n     c[i+1] = A[i]+B[i]+riporto;
    c*n     if c[i+1]<=1
    c*tif       riporto = 0;
    c*fif   else
    c*fif       riporto = 1;
    c*fif       c[i+1]=c[i+1]-2;
    c*1 C[1]=riporto;
\end{lstlisting}
Alla fine riduco in due casi dove se la somma dei due bit mi restituisce 1 o 0 allora
il riporto = 0, mentre se è > 1 setto il riporto a 1 e tolgo 2 dalla somma, perchè:
\begin{itemize}
    \item Se ho sommato 1 e 1 con riporto 0, ottengo 2 e ho riporto 1, togliendo 2 avrò 0 e in binario 1 + 1 + 0 = 10
    \item Se ho sommato 1 e 1 con riporto 1, ottengo 3 e ho riporto 1, togliendo 2 avrò 1 e in binario 1 + 1 + 1 = 11
\end{itemize}
\subsection{Calcolo tempo}
Come al solito tif è $t_if$ e indica quando l'if è vero e fif è $f_if$ e indica quando l'if
è falso.
\begin{equation*}
    t(n)=2c*1+3c*n+1c*t_if + 2c*f_if
\end{equation*}
\paragraph*{Caso migliore} Quando ho due vettori che sommati non danno mai riporto, quindi non esistono due
bit a 1 nella stessa posizione i.\\
\begin{align*}
    t_m(n)=2c+3cn+c*n+2c*0=\\
    =4cn+2c=\Omega(n)
\end{align*}
\paragraph*{Caso peggiore} Quando c'è sempre riporto, quindi quando:
\begin{center}
    A[n]=B[n]=1\\
    A[i] $\neq$ 0 e B[i] $\neq$ 0 per lo stesso i\\
    $\forall 1 \leq i \leq n-1$
\end{center}
\begin{align*}
    T_p(n) = 2c+3cn+c*0+2c*n=5cn+2c= O(n)
\end{align*}
\paragraph*{Confronto due casi}
Il caso migliore è effettivamente più veloce del peggiore dato che il migliore ha 4cn operazioni,
mentre il peggiore ha 5cn, ma hanno lo stesso ordine di grandezza, quindi per valori grandi non ci sarà
molta differenza.\\
In questo caso dato che il caso migliore e caso peggiore hanno lo stesso limite asintotico posso usare $\Theta$ 
e dire che il tempo dell'algoritmo sarà:
\begin{equation*}
    t_{somma}(n)=\Theta(n)
\end{equation*}
\paragraph*{Tempo medio} Quando il limite asintotico del caso peggiore e caso migliore coincide,
sicuramente il tempo medio avrà lo stesso limite asintotico, quindi non avrà molto senso calcolarlo, in
questo caso per esempio ci aspettiamo che in metà dei bit avrò riporto e in metà dei bit no, posso scrivere
la formula specifica, ma comunque otterrò sempre lo stesso limite asintotico, dato che se il limite superiore
e inferiore coincidono per ordine di grandezza, se cercao qualcosa in mezzo avrà necessariamente lo stesso
ordine di grandezza.
\section{Esercizio - Calcolo tempo codice già scritto}
\begin{lstlisting}[language=Java]
    int f-y(n)
    c   z=n;
    c   t=0;
    c log n while z>0
    c log n     x= z MOD 2
    c log n     z = z DIV 2
    c log n     if x == 0
    c tif*n         for i=1 to n
    c tif*n             t=t+1
    c   return(t)
\end{lstlisting}
\subsection{Calcolo tempo istruzioni}
Capiamo che il while viene eseguito $\log n$ volte perchè dipende da $z>0$ e
z viene diviso ogni volta per 2 (divisione intera) e questa è la tipica progressione
logaritmica. Infatti questo non è un vero e proprio while, dato che il numero di istruzioni eseguite
è determinato già all'inizio, potrebbe essere sostituito con un for.
\paragraph*{Tempo di esecuzione generico}
\begin{equation*}
    t(n)=3c+4c \log n + 2ct_{if} n
\end{equation*}
\paragraph*{Caso migliore} $t_if = 0$, cioè quando non entro mai nell'if e quindi quando ho
sempre resto nella divisione intera di z per 2, ma questo quando si verifica? Si verifica quando
dividendo per due un numero dispari, ottengo sempre un numero dispari. La divisione intera per due tenendo
da parte il resto non è altro che la procedura per convertire un numero decimale in un numero binario, quindi
ottengo sempre 1 quando sto convertendo un numero decimale che in binario è composto da solo 1 e questo si verifica
quando ho un numero del tipo $2^k - 1$. Esempi validi sono 63, 127, 1023, ecc.\\
\begin{align*}
    t_m(n)=3c+4c \log n + 0c = \Omega(\log n)
\end{align*}
\paragraph*{Caso peggiore} If sempre vero e quindi $n=2^k$
\begin{align*}
    t_{if}=\log n\\
    T_p=3c+4c\log n + 2cn*\log n = O(n\log n)
\end{align*}
In questo caso non posso usare $\Theta$ perchè i limiti asintotici del caso migliore e caso
peggiore sono diversi.
\paragraph*{Caso medio}
Il caso medio è quello dove per metà dei bit ho riporto e per metà no, quindi è dove eseguo il for
metà volta, quindi sarà $\frac{n \log n}{2}$.
\subsection{Miglioramento Algoritmo}
C'è un modo per migliorare questo algoritmo? Sì, il for è sostituibile con t=t+n.\\
Così facendo il caso peggiore (e quello medio) diventa:
\begin{equation*}
    O(n)=log n
\end{equation*}
\section{Esercizio for innestati}
\begin{lstlisting}[language=Java]
    f_x(n)
c       r=0
c(n-1)  for i=1 to n-1
sum(n-i)    for j=i+1 to n
sum(sum(j))     for k=1 to j
                    r=r+1
        return(r)
\end{lstlisting}
\subsection{Calcolo tempo}
In questo caso abbiamo 3 for annidati, con indici legati uno con l'altro.\\
In questi casi è necessario partire dal for più interno, il numero di volte per cui sarà eseguito sarà la
sommatoria del for sopra stante, se ha più di 1 for, ci sarà una sommatoria per ogni for, per intenderci,
il for più interno nel codice sopra riportato avrà la seguente sommatoria:
\begin{equation*}
    \sum_{i=1}^{n-1} (\sum_{j=i+1}^n j)
\end{equation*}
Analizziamo questa formula:
\begin{itemize}
    \item Il j più interno è il tempo di esecuzione del for più interno (for k=1 to j), dato che è un
    semplice incremento di variabile, quindi internamente va inserito il tempo di esecuzione del for più interno
    \item La sommatoria centrale riporta invece gli indici del for centrale, infatti parte da j = i+1 fino a n (come scritto
    nel for)
    \item Infine la sommatoria più esterna prende gli indici del primo for cioè parte da n fino ad arrivare a n-1
\end{itemize}
Questa formula rappresenta il numero di esecuzioni del for più interno.\\
\paragraph*{For centrale} In questo caso avrò una sola sommatoria $\sum_1^{n-1} n-1$
\paragraph*{For esterno} Come al solito in questo caso conto semplicemente quante volte viene eseguito
guardando fino a dove arrivare il contatore, in questo caso n-1.
\paragraph*{Risoluzione formula}
\begin{align*}
    t(n) = c + c(n-1) + c\sum_{i=1}^{n-1}(n-i) + 2c\sum_{i=1}^{n-1}\sum_{j=i+1}^n j + c\\
    \approx 2c+c*n+c\sum_{i=1}^{n-1}i+2c \dots =\\
    =2c+cn+c\frac{(n-1)(n)}{2} + 2c\sum_{i=1}^{n-1}\sum_{j=i+1}^n j + c\\
\end{align*}
\begin{itemize}
    \item Con l'approssimazione tolgo i -1 per esempio in $c(n-1)$, dato che non è rilevante.\\
    \item $\sum_{i=1}^{n-1}(n-i)$ se la leggo al contrario è come se stessi facendo \\
    $n-1, n-2, n-3, \dots, n-n$
    quindi la posso scrivere come $\sum_{i=1}^{n-1}i$ e quindi convertirla con la solita formula
    della sommatoria
    \item Per la doppia sommatoria devo cercare di convertire prima di tutto quella più interna
\end{itemize}
La doppia sommatoria la gestisco nella seguente maniera:
\begin{itemize}
    \item Devo convertire quella interna per scriverla in una forma che posso poi convertire in qualcosa
    contenente
    \item dato che ho $j=i+1$ posso pensare di riscriverla come $\sum_{j=1}^n j - (\sum_{j=1}^i j)$ e poi togliere
    tutte le i che sto trascurando (la sommatoria dopo il meno serve proprio a bilanciare la prima)
    \item Adesso posso convertire le due sommatorie come $\frac{n(n+1)}{2}-\frac{i(i+1)}{2}$, dato che
    nel secondo caso come indice in alto alla sommatoria ho i
    \item Quindi ora ho convertito la prima sommatoria e tornando alla formula iniziale ho
    $\sum_{i=1}^{n-1} (\frac{n(n+1)}{2}-\frac{i(i+1)}{2})$
    \item Ora posso fare delle approssimazioni, perchè tanto arriverò a scrivere un limite asintotico
    \item La prima parte è fissa dato che la sommatoria indica l'incremento di i, ma nel primo termine ho
    solo n, quindi avrò $\approx \sum_{i=1}^{n-1} \frac{n^2}{2} - \sum_{i=1}^{n-1} \frac{i(i+1)}{2}$
    \item La prima sommatoria la approssimo da i=1 a n (anzichè n-1), tanto il -1 non influenzerà l'ordine
    di grandezza, quindi avrò $\approx n*\frac{n^2}{2}$ mentre la seconda porto fuori il termine costante ($\frac{1}{2}$),
     $ -\frac{1}{2}\sum_{i=1}^{n-1}(i^2 + i)$
    \item Ora divido come prima i due termini in 2 sommatorie distinte $\approx \frac{n^3}{2} -\frac{1}{2}
    \sum_{i=1}^n i^2 + \sum_{i=1}^n i$ in entrambe le sommatorie il limite superiore $n-1$ è stato approssimato
    a $n$ per il solito discorso che -1 non influenza l'ordine di grandezza
    \item La seconda sommatoria so come convertirla, mentre per la prima introduciamo una nuova formula 
    (si trova in fondo a questo elenco), avrò quindi $\approx \frac{n^3}{2}-\frac{1}{2}(\frac{n(n+1)(2n+1)}{6}+ 
    \frac{n(n+1)}{2})$
    \item Così facendo ho risolto entrambe le sommatorie e ho solo n nella formula!
\end{itemize}
\paragraph*{Conversione importante da ricordare}
\begin{equation*}
    \sum_{i=1}^n i^2 = \frac{n(n+1)(2n+1)}{6}
\end{equation*}
\paragraph*{Conclusione risoluzione formula} Alla fine quindi avrò
\begin{align*}
    \approx \frac{n^3}{2}-\frac{1}{2}(\frac{n(n+1)(2n+1)}{6}+ 
    \frac{n(n+1)}{2})
\end{align*}
Ora che ho solo n posso fare ulteriori approssimazioni per poter cercare un limite asintotico
\begin{align*}
    \approx \frac{n^3}{2} -\frac{1}{2}\frac{2n^3}{6}-\frac{1}{4}n^2
\end{align*}
Ho considerato solo gli ordini di grandezza maggiori, in questo modo posso dedurre che 
\begin{align*}
    t(n)\simeq 2c+cn+\frac{cn^2}{2} + 2c(\frac{n^3}{2}-\frac{n^3}{6}-\frac{n^2}{4})\\
    \simeq 2c+cn+cn^2+2cn^3 = \Theta(n^3)
\end{align*}
Ho tolto sia le frazioni che i termini che vengono sottratti perchè comunque sono più piccoli dato
che il confronto è tra $\frac{n^3}{2}$ e $\frac{n^3}{6}$, dato che viene diviso per 6 è più piccolo
del primo, $n^2$ è proprio di un ordine inferiore.\\
\paragraph*{Non c'è caso migliore o peggiore} Questo perchè l'algoritmo ha un numero fisso di
istruzioni eseguite dato che ci sono solo for, NON ci sono condizioni variabili come if o while, infatti
scrivo $\Theta$.
\subsection{Quando usare le sommatorie}
Le sommatorie annidate una dentro l'altra sono da usare solo quando i for sono collegati da indici
in comune (in questo caso k,j,i erano collegati), se ho due for che non sono collegati uso semplicemente n per
indicare il numero di volte che vengono eseguiti, non userò la sommatoria.
\subsection{Note per l'esame per le approssimazioni}
Se sono palesi le approssimazioni possiamo anche non esplicitare cosa abbiamo fatto, ma se
saltiamo molti passaggi o facciamo approssimazioni non palesi dobbiamo esplicitare cosa abbiamo fatto scrivendo
due righe.
\section{Esercizio - Verifica se la matrice data è simmmetrica}
%\begin{lstlisting}[language=Java]

