\chapter{Digitalizzazione delle immagini}
Slide delle fasi dei Digital Image Fundamentals, siamo nella prima, digital world.\\
Immagini binarie, $x \in [0,\,1]$, appartiene ad una classe o all'altra.\\
Abstract? Immagini scala di grigio, la trasformo in array. chiedo a fabio\\
l(x,y)\in(a,b)\\
la digitalizzazione di immagini a colori è più complessa, la rappresento con 3 valori, una per ogni canale.\\
I canali non sono colori, sono visualizzati a livello di grigio (0-255) e in fase di visualizzazione esce il clore.

Altri tipi di immagini, satellitari ad es., che sono a 7 bande. Perché 7? La visione umana è a 3 bande, quindi 3 canali.\\
Oltre al visibile, potremmo essere interessati ad altre informazioni es. certi range per le onde elettromagnetiche (es. se l'uva è matura, se è inquinata, etc., cose non visibili a occhio).\\
\subsection{Nei video}
Abbiamo una dimensione in più. partiamo dai diversi frame che prendono ad es. le basse luci, le luci  medie e le luci alte che vengono poi combinate per creare le immagini Hcosa?\\
Perché utile? Perché nelle camere più moderne brevi sequenze vieo vengono combinate per creare immagini migliori.\\
Slide differenze immagini analogiche e digitali. La prima ha parametri spaziali che replicano le dimensioni reali, le seconde sono rappresentate da livelli di grigio.\\
\subsubsection{Acquisizione di immagini}
Due dispositivi:
\begin{itemize}
    \item dispositivi analogici
    \item dispositivi digitali
\end{itemize}
La qualità del colore per le immagini analogiche è migliore per quanto riguarda il colore.\\
quando dico che un'immagine è digitale, è digitalizzata nel dispositivo con cui la acquisisco.\\
\subsubsection{Caratterizzazione dell'immagine}
il processo di dormazione dell'immagine implica il mapping di una scena 3D in 2D, viene proiettata. Con questa proiezione rischio di perdere il senso di distanza (o lo perdo proprio sempre?) (slide digitalizzazione di immagini).\\
a meno che non si metta nell'immagine un riferimento assoluto che conosco per certo, non riesco a capire le reali dimensioni dell'oggetto. \`E quello che diceva nella prima lezione quando parlava delle iridi come campione di misura.
\subsection*{Digitalizzazione di immagini}
Le camere fotografiche digitali moderne sfruttano lo stesso concetto alla base della camera oscura.\\
\`E anche l'idea alla base della Polaroid. Perché quando togliamo la pellicola l'immagine diventa tutta grigia? Perché come ha detto Fabio non c'è la lente, non c'è la camera, perciò esce tutta grigia.\\
Ha nominato un algoritmo "Gray cosa?" che useremo per bilanciare le immagini.\\
La camera pinhole crea un'immagine riflessa dell'oggetto. Due slide su camera pinhole.\\
Potremmo fare un buco più grande per fare entrare più luce, ma non avendo la lente l'immagine esce molto sfocata.\\
Si usano quindi le \textbf{lenti}, che hanno una teoria molto complicata ma focalizza l'immagine.\\
Seguono slide utili per completezza. Una è su parametri geometrici, ottici, fotometrici\dots più ovviamente i parametri dei sensori.\\
\subsection*{I sensori}
Slide con la matrice grigia.\\
Ho un filtro, un vettore e un array.\\
Quando uso il filtro? Per togliere infrarossi e ultravioletti, riduce tutto ciò che non è luce visibile.\\
Quando uso il vettore (la linea)? Oggi si usa su ali dell'aereo o droni per fare foto aeree.\\
Quando uso la matrice? Boh.\\
Slide figura 2.13\\
\\
La risoluzione più alta ce l'ho a sinistra (che si muove) o quella a destra su cui ho messo i sensori?\\
Decisamente la prima perché la controllo io, la seconda è la dimensione del sensore. Lo scanner migliore (ha fatto l'esempio dello scanner a casa) è quello che ha la risoluzione più alta. Quindi sinistra.
\section{Segnale immagine o irradianza}
Questo è troppo semplificato sul libro. Slide con l'immagine della donna.\\
\`E il prodotto dell'illuminazione e della riflettanza.\\
Guarda la slide manca il secondo pezzo.\\

La riflettanza va da 0 a 1, l'illuminazione dipende dalla sorgente. Ma è \textit{sempre} fra 0 e 1? Ha fatto l'esempio della discoteca e dei raggi UV. Alcune sostanze, alcuni sbiancanti, assorbono, rielaborano e riemettono la luce dei raggi UV.\\
\section{Campionamento vs quantizzazione}
Slide presa dal Gonzales, c'è una matrice. Il prof mette l'origine nell'elemento in basso a sinistra, il libro in alto a sinistra. La slide successiva mostra i grafici differenze fra campionamento e quantizzazione, dove è possibile vedere il \textit{rumore}.\\
La slide successiva ne mostra la digitalizzazione. un pixel rappresenta quindi una regione dell'immgine e ha associato un solo valore di grigio. Ma è digitalizzato bene? Dipende dagli scopi della mia indagine.\\
\subsection{Campionamento}
Il sensore realizza un campionamento spaziale dell'immagine acquisita.\\
La cella del sensore del telefono deve diventare più piccolina perché devo tenere insieme i pixel, l'area sensibile diventa più piccola. Cosa vuol dire? Che perdo luce, prende più dettagli ma perde segnale. Quindi il mio sensore deve essere molto ma molto più sensibile.\\ 
Poco segnale $=$ poco rumore.\\
Non sempre la risoluzione spaziale è un parametro che va massimizzato.\\ 
\subsection{La risoluzione}
\`E un concetto relativo.\\ 
Tre tipi: 
\begin{itemize}
    \item r. dell'apparecchiatura di acquisizione
    \item r. dell'apparecchiatura di resa
    \item r. di ripresa di una scena
\end{itemize}
\subsubsection{Aliasing e campionamento}
Quando digitalizzo un'immagine, il campionamento in alto non è necessario.\\
Due slide, una è un buon esempio di campionamento, la seconda no. Si va a creare un'immagine che prima non avevo. \`E il fenomeno di Aliasing. Slide successiva con definizione. Poi c'è una slide con l'esempio di immagine corrotta per il fenomeno di Aliasing, esempio della persiana.\\
Ci sono teoremi che legano cosa al mio campionamento.\\
Il nostro campionamento deve essere almeno la metà della più piccola cosa che vogliamo sia visibile. Parliamo del teorema di Shannon. Qualsiasi segnale deve essere la somma di diverse curve, per essere sicuri di prendere tutti i più piccoli campionamenti.\\
Slide orologio da tasca: immagine divisa in 1250 punti.\\
\subsubsection{Campionamento e tecnologia dei sensori}
Slide risoluzione ottimale e tecnologia dei sensori.\\
Non va bene un sensore? Se ne cerca un altro.
\subsection{Vari modi per rappresentare un'immagine}
Slide con matrici.\\
Ci sono dimensioni più o meno standard.\\
Genericamente il più importante è il livello di quantizzazione. \`E il livello di grigio. \`E quasi sempre una potenza di due.\\
Es. slide della rosa: i pixel hanno sempre la stessa dimensione, ma cambia il numero di pixel. una 1024x1024 sarà grande e nitida, una 32x32 avrà grosse porzioni di immagine coperte da un solo pixel. Ovviamente se vado a prendere una piccola con pochi pixel es 32x32 e vado a ingrandirla, perdo dettagli e l'immagine uscirà molto molto sgranata. \`E la tecnica dietro all'oscuramento di volti alla tv. Ci sono algoritmi per simulare il recupero di definizione (visti alla magistrale), però sono una stima, una volta che l'informazione è persa è persa. \`E la slide di es. di elaborazione di basso livello, il lavoro che hanno fatto per la Canon, CNN con immagini di scimmia, farfalla, boh e bambino.\\
\subsection{Quantizzazione}
Quanti livelli di grigio servono?\\
La risoluzione di livelli di grigio è la più piccola differenza del livello di grigio è la più piccola che posso distinguere.\\ Slide con il tizio, i numeri sono il numero di livelli di grigio.
Con la quantizzazione prendiamo un numero finito di valori nel range $f(x,\,y)$.\\
Esempio della stampante: fa quantizzazione, partendo da 4 colori ne seleziona i livelli per andare ad ottenere milioni di colori. La quantizzazione entra in campo quando diminuisco la dimensione dei dot nella visualizzazione puntinata dell'immagine.
\subsubsection{Quantizzazione non uniforme}
Slide.
L'altra volta con la legge di Weber avevamo accennato alla quantizzazione, la discretizzazione dei livelli di grigio che non sempre opera su modelli lineari.\\
\subsubsection{Tempo di acquisizione}
Quando lo cambio cambio la quantità di luce che arriva al sensore. quando tengo un tempo troppo corto su immagine statica, l'immagine esce scura perché non entra abbastanza luce. Al contrario, quando è troppo alta va in saturazione e va a contaminare i pixel vicini (un po' quello che succedeva con la Polaroid).\\
In caso dinamico ovviamente il tempo di esposizione deve essere diverso. Per esempio per un treno in corsa il tempo di acquisizione deve essere molto basso, ha bisogno di un sensore più sensibile così che bastino meno fotoni per generare il segnale.
\subsubsection{Range dinamico}
Molto diverso dal numero di pixel (numero di livelli, che fa che è tipo un salame, ovvero non importa quante fette faccio ma quanto è lungo il salame), o dal numero di bit.\\
C'è la slide con la definizione che mi sono persa.\\
In base a quello che voglio prendere (esempio finestra in una stanza buia) cambia il range dinamico che devo prendere.\\
Come si risolve? con l'HDR dice Fabio? Mi serve che il sensore veda più luce nel buio e meno luce nella zona illuminata.\\
Cosa succede ? Negli scuri si vede più rumore, nelle zone troppo illuminate si satura.\\
\section{Digitalizzazione immagini a colori}
Per ogni pixel devo avere le 3 informazioni, per RGB. Per acquisire tutti e tre i canali potrei usare prismi. ma è complicato per molti motivi, tra cui un costo eccessivo per 3 sensori, oltre ad un'ottica troppo complicata. Come fa ad esempio un cellulare ad acquisire i colori per ogni singolo pixel? Ogni pixel può vedere un solo canale, poi vengono combinati. Riprendiamo il concetto di curva luminosa di efficienza. Il sensore acquisisce quello che si chiama un Bayer pattern, che viene interpolato.
\subsection{Lo spazio colore RGB}
Lo possiamo vedere come un cubo con i 3 colori sugli assi cartesiani, Dove saranno i grigi? Sulla diagonale che va dal punto nero (0, 0, 0) al punto bianco (255, 255, 255). Allontanandomi dalla diagonale dei grigi, vado verso le superfici del cubo che hanno i colori più saturi più carichi.\\
Come faccio ad editare i colori? Ruoto il cubo? Chiedi a fabio.\\
Slide con l'immagine colorata da scomporre in bande, seguono le versioni grigie e nere per ogni canale. Quella che rappresenta il canale verde è grigia e non bianca dove c'era il verde semplicemente perché non è il verde massimo.\\
Le immagini digitali possono esser visualizzate o riprodotte su appositi dispositivi quali monitor, display, proiettori e stampanti.\\
\\
Differenza fra visione additiva e sottrattiva. La visione umana è additiva, ma i colori primari che usiamo sono Red, Green e Blue, non rosso giallo e blu come ci hanno insegnato alle elementari. Il sistema RGB però è sottrattivo.
\subsubsection{--- Inizio Piccola Digressione ---}
Nella slide delle fasi dei Digital Image Fundamentals, l'editing rientra nella fase finale, mental world.\\
Parliamo di related e unrelated colours. Un esempio del secondo è che in una situazione di tutto buio il marrone si vede giallo, in luce si vede marrone, praticamente giallo scuro. Praticamente è il colore che vediamo in relazione ad un altro colore.
\subsubsection{--- Fine Piccola Digressione ---}

\subsection{HUI Hetc ho perso}
Ho perso la slide, quella con la definizione di HUI.\\

\subsection{Memorizzare i colori}
Slide con la matrice.

\section{Compressione delle immagini}
Standard più usato JPEG. Prima di fare una compressione, devo fare una trasformazione del colore.\\
Le singole bande vengono compresse per ridurre la ridondanza del segnale. Poi l'immagine viene decompressa e restituita. Il JPEG ha un fattore di imagine quality. Se andiamo ad aumentare il fattore di compressione, diventa più piccola l'immagine in fase di comprensione ma in fase di decompresione sui bordi vanno a crearsi degli artefatti.\\
Ovviamente il JPEG introduce gli artefatti ma è possibile con metodi e tecniche precise rimuovere questi artefatti. Parliamo di rastauro immagini compresse.
