\chapter{Programmazione non Lineare}
Un \textbf{problema di programmazione non lineare (PNL)} può essere formulato come: $$\text{opt}\, \mathit{f}(\mathbf{x})$$ soggetto ai seguenti vincoli: $$g_j(\mathbf{x})\, \le\, 0\quad \text{con}\quad j\, =\, 1, ..., m$$ $$x_i\, \ge\, 0\quad \text{con}\quad i\, =\, 1, ..., n$$ $\text{dove opt} = 
\begin{cases} 
        min \\ 
        max
\end{cases}
$
\\in cui $f\text{:}\; \R^n \to \R\,$ e $\, g_j\,\text{:}\; \R^n \to \R\,$ sono funzioni note di $x\, \in\, \R^N$.

\section{Tipologie di Programmazione non-lineare}
A seconda delle caratteristiche del problema abbiamo diversi tipi di programmazione non lineare.

\subsubsection{Ottimizzazione non vincolata}
I problemi di ottimizzazione non vincolata non hanno vincoli sulla regione ammissibile. Quindi l'obiettivo è semplicemente $$\text{max}\, f(\mathbf{x})\quad\, o\quad\, \text{min}\, f(\mathbf{x})\quad\, x\in\R^n$$.

\subsubsection{Ottimizzazione con vincoli lineari}
I problemi di ottimizzazione con vincoli lineari sono caratterizzati da tutte le funzioni $g_i(\mathbf{x})$ lineari, ma dalla funzione obiettivo non-lineare. Sono problemi che vanno a dare origine ad una \textit{regione ammissibile convessa}. Un caso particolare è la \textit{programmazione quadratica}, in cui la funzione obiettivo è una funzione quadratica.

\subsubsection{Ottimizzazione convessa}
Problemi in cui $f(\mathbf{x})$ è una funzione \textbf{concava} o \textbf{convessa} e ogni funzione $g_i(\mathbf{x})$ è \textbf{convessa}.

\subsubsection{Ottimizzazione non convessa}
I problemi di programmazione non convessa comprendono tutti i problemi che non soddisfano le ipotesi di convessità. Sono più difficili da risolvere perché possono presentare diversi punti di minimo/massimo.

\section{Gradiente ed Hessiano}
Vogliamo estendere i concetti di derivata (prima e seconda) visti per funzioni in una variabile, al caso di funzioni in più variabili.
\\In particolare, considereremo lo spazio euclideo $\R^2$ (ovvero lo spazio vettoriale $\R^2$ dotato di norma euclidea), da cui sarà possibile facilmente generalizzare allo spazio $\R^n$ con $n\, >\, 2$.
\\Sia $A \subset \R^2$ e $f:\, A \rightarrow \R$ una funzione definita sull'insieme $A$.

\subsection{Le derivate parziali}

\subsubsection{Def. Derivata Direzionale}
Supponiamo di voler calcolare la velocità di crescita della funzione $f(x,\, y)$ nel punto $(x_0,\, y_0)$ quando ci si muove nella direzione del vettore $v = (a,\, b)$. I punti di coordinate $(x_0 + ha,\, y_0 + hb)$ descrivono, al variare di $h$ in $\R$, la retta nel piano $\widehat{xy}$ passante per il punto $(x_0,\, y_0)$ e parallela a $v$.
\\La derivata direzionale di $f$ in $(x_0,\, y_0)$ nella direzione individuata dal versore $v\, =\, (a,\, b)$ (con $\| v\|\, =\, 1$) si indica con $D_v\, f(x_0,\, y_0)$ e vale $$D_v\, f(x_0,\, y_0)\, =\, \lim_{h\to 0}\frac{f(x_0 + ha,\, y_0 + hb)\, -\, f(x_0,\, y_0)}{h} $$ (se il limite esiste).

\subsubsection{Def. Derivata Parziale}
Le derivate parziali di $f$ nel punto $(x_0,\, y_0)\in A$ sono definite come $$\frac{\partial f(x_0,\, y_0)}{\partial x}\, =\, \lim_{h\to 0}\frac{f(x_0 + h,\, y_0)\, -\, f(x_0,\, y_0)}{h} ,$$ $$\frac{\partial f(x_0,\, y_0)}{\partial y}\, =\, \lim_{h\to 0}\frac{f(x_0,\, y_0 + h)\, -\, f(x_0,\, y_0)}{h} ,$$ purché i limiti esistano finiti. Quindi, le derivate parziali corrispondono alla derivata direzionale rispetto ai vettori $v_1 = (1,\, 0)$ e $v_2 = (0,\, 1)$, rispettivamente.

\subsubsection{Varianti di notazioni per le derivate parziali}
$\frac{\partial f(x_0,\, y_0)}{\partial x},\, \quad \partial_x f(x_0,\, y_0),\, \quad f_x(x_0,\, y_0),\, \quad \frac{\partial}{\partial_x} f(x_0,\, y_0)$.

\subsubsection{Comportamenti delle derivate parziali}
\begin{itemize}
    \item Se $\frac{\partial f(x_0,\, y_0)}{\partial x} > 0$, allora la funzione è crescente lungo la direzione $x$.
    \item Se $\frac{\partial f(x_0,\, y_0)}{\partial y} > 0$, allora la funzione è crescente lungo la direzione $y$.
    \item Se $\frac{\partial f(x_0,\, y_0)}{\partial x} < 0$, allora la funzione è crescente lungo la direzione $x$.
    \item Se $\frac{\partial f(x_0,\, y_0)}{\partial y} < 0$, allora la funzione è crescente lungo la direzione $y$.
\end{itemize}
$\frac{\partial f(x_0,\, y_0)}{\partial x},\, \quad \partial_x f(x_0,\, y_0),\, \quad f_x(x_0,\, y_0),\, \quad \frac{\partial}{\partial_x} f(x_0,\, y_0)$.

\subsubsection{Derivate di ordine successivo}
Sia $f(x,\, y)$ una funzione derivabile in $A\, \subset\, \R^2$, ovvero siano definite in $A$ le 2 derivate parziali $\frac{\partial f(x,\, y)}{\partial x}$ e $\frac{\partial f(x,\, y)}{\partial y}$. Ogni derivata parziale è una funzione definita in $A$. Se $\frac{\partial f(x,\, y)}{\partial x}$ è a sua volta derivabile, le sue derivate parziali $$\frac{\partial^2 f(x,\, y)}{\partial x \partial x},\quad \frac{\partial^2 f(x,\, y)}{\partial x \partial y}$$ sono dette \textbf{derivate seconde}. In modo simile possono essere definite le derivate parziali di $\frac{\partial f(x,\, y)}{\partial y}$, ovvero $$\frac{\partial^2 f(x,\, y)}{\partial y \partial x},\quad \frac{\partial^2 f(x,\, y)}{\partial y \partial y}$$

\subsection{Gradiente di una funzione}
Il gradiente di $f(x, y)$ è per definizione il vettore $\text{D}f$ le cui componenti sono le \textbf{derivate parziali} di $f$, ovvero $$\nabla f(x,\, y) = \left[\frac{\partial f(x,\, y)}{\partial x},\, \frac{\partial f(x,\, y)}{\partial y}\right]^T$$
\\Anche in questo caso esistono varie notazioni per indicare il gradiente: $$\nabla f,\quad \text{D}f(x,\, y),\quad \nabla f(x,\, y),\quad \text{grad} f$$
\\Gli elementi sulla diagonale principale vengono chiamati \textbf{derivate pure}, per distinguerli dagli altri elementi chiamati \textbf{derivate miste}.

\subsection{Hessiano di una funzione}
L'insieme delle derivate seconde costituisce una matrice quadrata chiamata matrice Hessiana, e si denota con il simbolo $\text{D}^2 f$ o $H_f$.
$$H_f f(x,\, y) = \left[\begin{matrix}
    \frac{\partial^2 f(x,\, y)}{\partial x \partial x} & \frac{\partial^2 f(x,\, y)}{\partial x \partial y} \\
    \frac{\partial^2 f(x,\, y)}{\partial y \partial x} & \frac{\partial^2 f(x,\, y)}{\partial y \partial y}
\end{matrix}\right]$$

\subsection{Legami fra gradiente, hessiano e punti di ottimo}
Il gradiente riassume le informazioni di crescita della funzione lungo tutte le direzioni. I punti per cui il gradiente si annulla ($\nabla f = 0$), prendono il nome di \textbf{punti stazionari}.

\subsubsection{Legame tra stazionarietà e gradiente}
Se $f:\, A \subset \R^2 \rightarrow \R$ è derivabile in un punto $(x_0,\, y_0)$ di massimo o di minimo locale, allora $$\nabla f(x_0,\, y_0) = \left[\frac{\partial f(x,\, y)}{\partial x},\, \frac{\partial f(x,\, y)}{\partial y}\right]^T = [0,\, 0]^T$$
\\N.B.: 
\begin{itemize}
    \item nel caso 1D (le derivate tradizionali fatte finora, ad un'incognita e quindi una dimensione) un \textbf{punto stazionario} ovvero per $\mathbf{f'(x)\, =\, 0}$ può essere un punto di \textbf{massimo}, di \textbf{minimo} (o di flesso);
    \item nel caso 2D visto nei paragrafi precedenti un \textbf{punto stazionario} ovvero per $\mathbf{\nabla f(x)\, =\, 0}$ può essere \textbf{anche} un punto di \textbf{sella}.
\end{itemize}
Un punto di sella è un punto in cui, ad esempio, lungo la direzione $x$ si comporta come \textit{minimo} (ovvero la funzione è convessa lungo la direzione $x$) e lungo la direzione $y$ si comporta come \textit{massimo} (ovvero la funzione è concava lungo la direzione $y$).

\subsubsection{Def. matrice definita positiva/negativa}
Cosa significa matrice definita positiva e negativa?
\\
\\Una matrice quadrata $A \in \R^2 \times \R^2$ è \textbf{definita positiva} se $$x^T Ax > 0\quad \forall x \in \R^2 \setminus 0$$ oppure se tutti gli autovalori della matrice sono positivi.
\\
\\Una matrice quadrata $A \in \R^2 \times \R^2$ è \textbf{semi definita positiva} se $$x^T Ax \geq 0\quad \forall x \in \R^2 \setminus 0$$
Una matrice quadrata $A \in \R^2 \times \R^2$ è \textbf{definita negativa} se $$x^T Ax < 0\quad \forall x \in \R^2 \setminus 0$$ oppure se tutti gli autovalori della matrice sono negativi. 
\\
\\Una matrice quadrata $A \in \R^2 \times \R^2$ è \textbf{semi definita negativa} se $$x^T Ax \leq 0\quad \forall x \in \R^2 \setminus 0$$
\\Ma come si trovano?

\subsubsection{Trovare gli autovalori di una matrice}
Premessa scontata ma dovuta: la nozione di autovalore si riferisce \textbf{solo} alle matrici \textbf{quadrate}.
\\Sia $A$ una matrice di ordine $n$, ovvero $A \in \mathbb{K}^n$, dove $\mathbb{K} \subseteq \mathbb{R}$ o anche $\mathbb{K} \subseteq \mathbb{C}$.
\\Si dice che lo scalare $\lambda_0 \in \mathbb{K}$ è un \textbf{autovalore} della matrice $A$ se $\exists$ un vettore colonna \textbf{non nullo} $\mathbf{v} \in \mathbb{K}^n$ tale che $$A\mathbf{v} = \lambda_0\mathbf{v}$$ dove $\mathbf{v}$ è detto \textbf{autovettore relativo all'autovalore} $\lambda_0$.
\\Da questa uguaglianza possiamo ricavare che $$A\mathbf{v} - \lambda_0\mathbf{v} = 0$$ e di conseguenza $$(A - \lambda_0\mathbf{Id}_n)\mathbf{v} = 0$$ dove con $\mathbf{Id}_n$ indichiamo la \textbf{matrice identità} dello stesso ordine di $A$.\\
\\Questa è la forma matriciale di un \textit{sistema lineare omogeneo}. Dalla teoria sui \textit{sistemi lineari} sappiamo che un sistema omogeneo ammette una soluzione diversa dalla soluzione banale \textit{sse} la matrice incompleta (anche detta \textit{matrice dei coefficienti}, utile per il teorema di Rouché-Capelli) associata al sistema ha \textbf{determinante uguale a 0}, quindi $$\text{det}(A - \lambda_0\text{Id}_n) = 0$$ {\footnotesize (avendo assunto $\mathbf{v}$ non nullo all'inizio, possiamo non trascriverlo)}\\
\\Se consideriamo $\lambda$ come un'incognita, otteniamo quello che è chiamato per definizione \textbf{polinomio caratteristico} associato alla matrice $A$. Si deduce che \textcolor{blue}{gli \textbf{autovalori} di una matrice sono \textbf{gli zeri del polinomio caratteristico}}.

\subsubsection{Legame tra hessiano e convessità}
Sia $f(x,\, y)$ una funzione derivabile due volte, allora:
\begin{itemize}
    \item $f$ è convessa nell'insieme A se e solo se $H_f (x,\, y)$ è semi definita positiva.
    \item $f$ è concava nell'insieme A se e solo se $H_f (x,\, y)$ è semi definita negativa.
\end{itemize}

\subsubsection{Legame tra hessiano, gradiente e punti di ottimo}
Sia $f: A \subset \R^2 \rightarrow \R$ una funzione continua in $A$ e avente derivate prime e seconde continue in $A$. 
\\Condizione \textcolor{blue}{\textbf{sufficiente}} affinché (x,\, y) sia un punto di \textcolor{blue}{\textit{minimo}} per $f$ è che:
\begin{itemize}
    \item $\nabla f(x,\, y) = \mathbf{0}$ (derivate parziali tutte \textbf{nulle})
    \item $H_f (x,\, y)$ sia definita \textcolor{blue}{\textbf{positiva}}
\end{itemize}
Condizione \textcolor{blue}{\textbf{sufficiente}} affinché (x,\, y) sia un punto di \textcolor{blue}{\textit{massimo}} per $f$ è che:
\begin{itemize}
    \item $\nabla f(x,\, y) = \mathbf{0}$ (derivate parziali tutte \textbf{nulle})
    \item $H_f (x,\, y)$ sia definita \textcolor{blue}{\textbf{negativa}}
\end{itemize}


\section{Massimizzazione di funzioni concave}
Consideriamo un problema di massimizzazione con una funzione obiettivo $f(x)$ concava da massimizzare.
Essendo questa funzione \textbf{concava}, sappiamo che una condizione sufficiente affinchè $x^*$ sia \textbf{punto di massimo} è che:
\[
    \frac{d}{dx}f(x^*) = 0 \text{, ovvero il punto di massimo è dove la derivata è stazionaria}
\]
Capiamo qundi che se \emph{un'equazione può essere risolta analiticamente} allora il procedimento per trovare l'ottimo termina.
\\Un discorso equivalente si può fare per problemi di minimizzazione di funzioni convesse.
\subsection{Algoritmi per la risoluzione numerica}
E se non posso risolverla analiticamente?
In mancanza di una risoluzione analitica (per esempio per una funzione troppo complicata) sono disponibili algoritmi per la risoluzione numerica del problema.

\subparagraph{L'idea} degli algoritmi per la risoluzione numerica:
Si costruisce una sequenza di punti $\{x_k\}$ t.c.: (nei casi di minimizzazione)
\[\lim_{x\to +\infty} x_k = x^* \text{ e } f(x_{k+1}) \leq f(x_k)\]
Ad ogni iterazione $k$, partendo da $x_k$ si esegue una ricerca sistematica per identificare un punto migliore $x_k$.

\paragraph{I criteri di Arresto}
A differenza dell'algoritmo del simplesso, in questo caso la sequenza di punti $\{x_k\}$ non è detto che converga alla soluzione ottima del problema in un numero finito di iterazioni.

Quindi quando fermo la sequenza di punti?
\begin{itemize}
    \item La soluzione è sufficientemente accurata, ovvero $\frac{df(x_k)}{dx} \simeq 0$
    \item Quando si è raggiunto un numero massimo di iterazioni $N$ o un tempo computazionale massimo.
    \item I progressi sono lenti, ovvero $|x_{k+1} - x_k| < \epsilon_x$ o  $|f(x_{k+1}) - f(x_k)| < \epsilon_f$.
    \item La soluzione diverge.
    \item Si verificano cicli.
\end{itemize}

\subsection{Gli algoritmi}
Esistono due tipi di algoritmi:
\\\textbf{Dicotomici}: Algoritmi di ricerca per individuare un determinato valore (per il quale la funzione derivata si annulla) all'interno di un intervallo che ad ogni iterazione viene ridotto.
\\\textbf{Di approssimazione}: utilizzando approssimazioni locali della funzione.

Per l'ottimizzazione di funzioni in una variabile tratteremo:
\begin{itemize}
    \item Metodo di Bisezione (Dicotomico)
    \item Metodo di Newton (Di Approssimazione)
\end{itemize}

\section{Il metodo di Bisezione}
L'idea di questo algoritmo è:
\\Se $f(x)$ è \emph{continua e concava} in un intervallo chiuso $[a.b]$ allora, considerando \textbf{un generico punto $x_k$}, se:
\begin{itemize}
    \item $\frac{d}{dx} f(x_k) < 0 \implies $ l'ottimo $x^*$ si trova a \textbf{sinistra} di $x_k$.
    \item $\frac{d}{dx} f(x_k) > 0 \implies $ l'ottimo $x^*$ si trova a \textbf{destra} di $x_k$.
    \item $\frac{d}{dx} f(x_k) \simeq 0 \implies $ $x_k \simeq x^*$, ovvero è circa l'ottimo.
\end{itemize}

\paragraph*{Il Metodo}
Il metodo di bisezione è una procedura abbastanza intuitiva.
Guardando la pendenza della derivata per un punto $x_k$ in una funzione \emph{concava e derivabile:}
\begin{itemize}
    \item Se $f'(x_k) > 0$:
    \begin{itemize}
        \item Il nuovo punto $x_{k+1}$ lo ottengo spostandomi verso Destra
        \item $x_k$ rappresenta un \emph{Estremo Inferiore} $\underline{x}$ per il punto $x^*$
    \end{itemize}
    \item Se $f'(x_k) < 0$:
    \begin{itemize}
        \item Il nuovo punto $x_{k+1}$ lo ottengo spostandomi verso Sinistra
        \item $x_k$ rappresenta un \emph{Estremo Superiore} $\bar{x}$ per il punto $x^*$
    \end{itemize}
\end{itemize}

Ad ogni iterazione $k$ posso identificare un sotto-intervallo di ricerca $[a_k, b_k] \subset [a.b]$
per ridurre lo spazio di ricerca in modo da identificare un'iterazione $k$ per cui $|b_k - a_k| < 2\epsilon$ 
con $|a_k - x^*| < \epsilon$ e $|x^*-b_k|<\epsilon$

\subsection{L'algoritmo}
\paragraph*{Inizializzazione}
$k=0$ e $\epsilon$ piccolo a piacere.
\\Si determinano i valori $\underline{x}$ e $\bar{x}$ per cui la derivata sia rispettivamente positiva e negativa.
\\Si seleziona il punto iniziale $x_0 = \frac{\underline{x} + \bar{x}}{2}$

\paragraph*{Iterazione del metodo di bisezione} bisogna: \\
\begin{enumerate}
    \item Calcolare $f'(x_k)$
    \item 
\begin{lstlisting}[mathescape=true]
if $f'(x)=0$ then $x_k = x^*$
else
    if $f'(x)<0$
        $\bar{x} = x_k$
    else
        $\underline{x} = x_k$
\end{lstlisting}
    \item Pongo $x_{k+1} = \frac{\underline{x} + \bar{x}}{2}$ e $k=k+1$ 
\end{enumerate} 

Criterio di arresto: se $underline{x} + \bar{x} \leq 2\epsilon$ allora il punto di ottimo $x^*$ avrà una distanza minore di $\epsilon$ da uno dei due. 

\section*{il Metodo di Newton}
Il metodo di newton è molto simile a quello di bisezione ma  considera anche la derivata seconda e non solo l'informazione relativa alla derivata prima.

\paragraph*{L'approssimazione quadratica}
il Metodo di newton ottiene un'approssimazione quadratica (in $x^2$) tramite la formula di Taylor centrata in $x_k$
\[
    f(x_k) \sim f(x_k) + f'(x_k)(x_{k+1} - x_k) + \frac{1}{2} f''(x_k)(x_{k+1} - x_k)^2
\]
L'approsimazione quadratica è in funzione solo di $x_{k+1}$!
Calcoliamo la derivata dell'approssimazione quadratica
\[
    f'(x_k)+f''(x_k)(x_{k+1} - x_k)= 0 \to x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}
\]

Quindi, l'idea dell'algoritmo di Newton è quella di usare l'ottimo dell'approsimazione quadratica di $f(x)$ dato da:
\[
    x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}
\]
Questa formula viene usata ad ogni generiaca iterazione $k$ per calcolare il punto successivo $x_{k+1}$

\paragraph*{Importante}
Se $f(x)$ è concava allora $x_k$ converge verso un punto di massimo, perchè?
\begin{itemize}
    \item $x_k$ è a Sinistra del punto di massimo
    \\ $\implies f'(x_k) > 0 \implies -\frac{f'(x_k)}{f''(x_k)}>0 \implies x_{k+1}> x_k$
    \item $x_k$ è a Destra del punto di massimo
    \\ $\implies f'(x_k) < 0 \implies -\frac{f'(x_k)}{f''(x_k)}<0 \implies x_{k+1}< x_k$
\end{itemize}

\subsection{L'algoritmo di newton}
\begin{itemize}
    \item Inizializzazione
    \begin{itemize}
        \item si fissa $\epsilon$ e $k=0$ 
    \end{itemize} 
    \item Iterazione del metodo di newton
    \begin{itemize}
        \item Calcola $f'(x_k)$ e $f''(x)$
        \item Si pone $x_{k+1} = x_k -  \frac{f'(x_k)}{f''(x_k)}$
    \end{itemize} 
\end{itemize}
Criterio di arresto: 