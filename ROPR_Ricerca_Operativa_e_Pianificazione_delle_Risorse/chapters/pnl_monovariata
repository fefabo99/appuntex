\chapter{PNL Monovariata}

\section{Risoluzione Analitica}
Consideriamo un problema di massimizzazione con una funzione obiettivo $f(x)$ concava da massimizzare.
Essendo questa funzione \textbf{concava}, sappiamo che una condizione sufficiente affinchè $x^*$ sia \textbf{punto di massimo} è che:
\[
    \frac{d}{dx}f(x^*) = 0 \text{, ovvero il punto di massimo è dove la derivata è stazionaria}
\]
Capiamo qundi che se \emph{un'equazione può essere risolta analiticamente} allora il procedimento per trovare l'ottimo termina.
\\Un discorso equivalente si può fare per problemi di minimizzazione di funzioni convesse.
\section{Algoritmi Generici per la Risoluzione Numerica}
E se non posso risolverla analiticamente?
In mancanza di una risoluzione analitica (per esempio per una funzione troppo complicata) 
Esistono degli \textbf{algoritmi per la risoluzione numerica} che si basano sul concetto di sequenza.

\paragraph{Gli algoritmi della sequenza}
Si costruisce una sequenza di punti $\{x_k\}$ t.c.: (nei casi di minimizzazione)
\[\lim_{x\to +\infty} x_k = x^* \text{ e } f(x_{k+1}) \leq f(x_k)\]
Ad ogni iterazione $k$, partendo da $x_k$ si esegue una ricerca sistematica per identificare un punto migliore $x_k$.

\subparagraph{I criteri di Arresto}
A differenza dell'algoritmo del simplesso, in questo caso la sequenza di punti $\{x_k\}$ non è detto che converga alla soluzione ottima del problema in un numero finito di iterazioni.
Quindi quando fermo la sequenza di punti? Mi fermo se:
\begin{itemize}
    \item La soluzione è sufficientemente accurata, ovvero $\frac{df(x_k)}{dx} \simeq 0$
    \item Quando si è raggiunto un numero massimo di iterazioni $N$ o un tempo computazionale massimo.
    \item I progressi sono lenti, ovvero $|x_{k+1} - x_k| < \epsilon_x$ o  $|f(x_{k+1}) - f(x_k)| < \epsilon_f$.
    \item La soluzione diverge.
    \item Si verificano cicli.
\end{itemize}
\subsubsection*{Gli algoritmi}
Esistono due tipi di algoritmi che si basano su questo funzionamento:
\begin{description}
    \item[Dicotomici]:Algoritmi di ricerca per individuare un determinato valore (per il quale la funzione derivata si annulla) all'interno di un intervallo che ad ogni iterazione viene ridotto.
    \item[Di Approssimazione]: Algoritmi che utilizzando approssimazioni locali della funzione.
\end{description}
Per l'ottimizzazione di funzioni in una variabile tratteremo il \emph{Metodo di Bisezione}, che é di tipo Dicotomico,
e il  Metodo di Newton, che é un algoritmo di Approssimazione.

\section{Metodo di Bisezione}
Il metodo di Bisezione é una procedura intuitiva e semplice che puó essere applicata ad un problema PNL non vincolato quando
$f(x)$ é concava/convessa, continua e derivabile.
L'idea di questo algoritmo è la seguente:
\\Se $f(x)$ è \emph{continua e concava} in un intervallo chiuso $[a.b]$ allora, considerando \textbf{un generico punto $x_k$}, se:
\begin{itemize}
    \item $\frac{d}{dx} f(x_k) < 0 \implies $ l'ottimo $x^*$ si trova a \textbf{sinistra} di $x_k$.
    \item $\frac{d}{dx} f(x_k) > 0 \implies $ l'ottimo $x^*$ si trova a \textbf{destra} di $x_k$.
    \item $\frac{d}{dx} f(x_k) \simeq 0 \implies $ $x_k \simeq x^*$, ovvero è circa l'ottimo.
\end{itemize}

\subsection{Funzionamento}
Il metodo di bisezione è una procedura abbastanza intuitiva.
Guardando la \emph{pendenza della derivata} per un punto $x_k$ in una funzione \emph{concava e derivabile}:
\begin{itemize}
    \item Se $f'(x_k) > 0$:
    \begin{itemize}
        \item Il nuovo punto $x_{k+1}$ lo ottengo spostandomi verso Destra
        \item $x_k$ rappresenta un \emph{Estremo Inferiore} $\underline{x}$ per il punto $x^*$
    \end{itemize}
    \item Se $f'(x_k) < 0$:
    \begin{itemize}
        \item Il nuovo punto $x_{k+1}$ lo ottengo spostandomi verso Sinistra
        \item $x_k$ rappresenta un \emph{Estremo Superiore} $\bar{x}$ per il punto $x^*$
    \end{itemize}
\end{itemize}
Ad ogni iterazione $k$ posso identificare un sotto-intervallo di ricerca $[a_k, b_k] \subset [a.b]$
per ridurre lo spazio di ricerca in modo da identificare un'iterazione $k$ per cui $|b_k - a_k| < 2\epsilon$ 
con $|a_k - x^*| < \epsilon$ e $|x^*-b_k|<\epsilon$

\subsection{Algoritmo}
\paragraph{Inizializzazione}
$k=0$ e $\epsilon$ piccolo a piacere.
\\Si determinano i valori $\underline{x}$ e $\bar{x}$ per cui la derivata sia rispettivamente positiva e negativa.
\\Si seleziona il punto iniziale $x_0 = \frac{\underline{x} + \bar{x}}{2}$

\paragraph{Iterazione del metodo di bisezione} bisogna:
\begin{enumerate}
    \item Calcolare $f'(x_k)$
    \item 
\begin{lstlisting}[mathescape=true]
if $f'(x)=0$ then $x_k = x^*$
else
    if $f'(x)<0$
        $\bar{x} = x_k$
    else
        $\underline{x} = x_k$
\end{lstlisting}
    \item Pongo $x_{k+1} = \frac{\underline{x} + \bar{x}}{2}$ e $k=k+1$ 
\end{enumerate} 

Criterio di arresto: se $\underline{x} + \bar{x} \leq 2\epsilon$ allora il punto di ottimo $x^*$ avrà una distanza minore di $\epsilon$ da uno dei due. 

\section{Metodo di Newton}
Il metodo di newton è molto simile a quello di bisezione ma  considera anche la derivata seconda e non solo l'informazione relativa alla derivata prima.

\subsection{Funzionamento}
il Metodo di newton ottiene un'approssimazione quadratica (in $x^2$) tramite la formula di Taylor centrata in $x_k$
\[
    f(x_k) \sim f(x_k) + f'(x_k)(x_{k+1} - x_k) + \frac{1}{2} f''(x_k)(x_{k+1} - x_k)^2
\]
L'approsimazione quadratica è in funzione solo di $x_{k+1}$!
Calcoliamo la derivata dell'approssimazione quadratica
\[
    f'(x_k)+f''(x_k)(x_{k+1} - x_k)= 0 \to x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}
\]

Quindi, l'idea dell'algoritmo di Newton è quella di usare l'ottimo dell'approsimazione quadratica di $f(x)$ dato da:
\[
    x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}
\]
Questa formula viene usata ad ogni generiaca iterazione $k$ per calcolare il punto successivo $x_{k+1}$

\paragraph{Importante}
Se $f(x)$ è concava allora $x_k$ converge verso un punto di massimo, perchè?
\begin{itemize}
    \item $x_k$ è a Sinistra del punto di massimo
    \\ $\implies f'(x_k) > 0 \implies -\frac{f'(x_k)}{f''(x_k)}>0 \implies x_{k+1}> x_k$
    \item $x_k$ è a Destra del punto di massimo
    \\ $\implies f'(x_k) < 0 \implies -\frac{f'(x_k)}{f''(x_k)}<0 \implies x_{k+1}< x_k$
\end{itemize}

\subsection{Algoritmo}
\begin{itemize}
    \item Inizializzazione
    \begin{itemize}
        \item si fissa $\epsilon$ e $k=0$ 
    \end{itemize} 
    \item Iterazione del metodo di newton
    \begin{itemize}
        \item Calcola $f'(x_k)$ e $f''(x)$
        \item Si pone $x_{k+1} = x_k -  \frac{f'(x_k)}{f''(x_k)}$
    \end{itemize} 
\end{itemize}
Criterio di arresto: 