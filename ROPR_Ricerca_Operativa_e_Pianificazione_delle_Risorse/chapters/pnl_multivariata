\chapter{PNL Multivariata}
Nella PNL Monovariata utilizziamo le informazioni di Derivata Prima e Seconda per trovare l'ottimo.
Anche nel caso della PNL Multivariata usiamo gli stessi concetti, bisogna però estenderli agli spazi multidimensionali.
\\Vogliamo estendere i concetti di derivata (prima e seconda) visti per funzioni in una variabile, al caso di funzioni in più variabili.
\\In particolare, consideriamo uno spazio $\R^n$, ovvero uno spazio vettoriale a $n$ dimensioni.
Se nel caso $\R^1$ avevamo Derivata I e II, nel caso $n$-dimensionale:
\begin{itemize}
    \item $f'(x) \to$ Gradiente.
    \item $f''(x) \to$ Hessiana.
\end{itemize} 

\section{Derivate Parziali}

Per conoscere Gradiente ed Hessiana dobbiamo introdurre il concetto di Derivata Parziale.

\definizione{
    Le \textbf{Derivate Parziali} (anche dette derivate direzionali) sono le derivate in un iperspazio per una determinata direzione $v$.
    \paragraph{Quindi} se ho $f(x,y,z)$ posso ottenere
    \[
        \frac{d f(x,y,z)}{dx}, \frac{d f(x,y,z)}{dy}, \frac{d f(x,y,z)}{dz}
    \]
    Ovvero le derivate parziali di $f(x,y,z)$ in ognuna delle sue variabili.
    \linebreak \small{
        \nb{Se derivo $f(x,y,z)$ in $x$, allora $y$ e $z$ vengono trattate come costanti nel processo di derivazione.}
    }
}

\section{Gradiente ed Hessiana}
Ora possiamo quindi introdurre Gradiente ed Hessiana:
\subsection{Gradiente}
Il gradiente di $f(x, y)$ è per definizione il vettore $\text{D}f$ le cui componenti sono le \textbf{derivate parziali} di $f$, ovvero
$$\nabla f(x,\, y) = \left[\frac{\partial f(x,\, y)}{\partial x},\, \frac{\partial f(x,\, y)}{\partial y}\right]^T$$
Esistono varie notazioni per indicare il gradiente: $$\nabla f,\quad \text{D}f(x,\, y),\quad \nabla f(x,\, y),\quad \text{grad} f$$

\subsection{Hessiana}
L'insieme delle derivate seconde costituisce una matrice quadrata chiamata matrice Hessiana, e si denota con il simbolo $\text{D}^2 f$ o $H_f$.
$$H_f f(x,\, y) = \left[\begin{matrix}
    \frac{\partial^2 f(x,\, y)}{\partial x \partial x} & \frac{\partial^2 f(x,\, y)}{\partial x \partial y} \\
    \frac{\partial^2 f(x,\, y)}{\partial y \partial x} & \frac{\partial^2 f(x,\, y)}{\partial y \partial y}
\end{matrix}\right]$$

In cui ogni riga della matrice Hessiana "corrisponde" ad un elemento del Gradiente, 
mentre ogni colonna corrisponde alla derivata parziale in ognuna delle variabili di $f$.
\paragraph*{Derivate Pure}
Gli elementi sulla diagonale principale vengono chiamati \textbf{derivate pure}, per distinguerli dagli altri elementi chiamati \textbf{derivate miste}.


\section{Metodo Analitico per l'Ottimo}
Il gradiente riassume le informazioni di crescita della funzione lungo tutte le direzioni. I punti per cui il gradiente
si annulla ($\nabla f = 0$), prendono il nome di \textbf{punti stazionari}.

\subsection{Punti stazionari n-dimensionali}
\definizione{
    Se abbiamo un punto $(x_0,\, y_0)$ t.c. 
    \[
        \nabla f(x_0,\, y_0) = \left[\frac{\partial f(x,\, y)}{\partial x},\, \frac{\partial f(x,\, y)}{\partial y}\right]^T = [0,\, 0]^T
    \]
    Allora questo è un \textbf{punto stazionario}, e potrebbe essere un punto di massimo, minimo o di sella.
}

\paragraph*{Punto di Sella}
Se nel caso 1D ogni punto in cui la derivata si annulla è un punto di Max o Min, nel caso n-dimensionale puó anche essere un \emph{punto di sella}, ovvero un punto che è massimo in una direzione (variabile) e minimo nell'altra.

\subsection{Trovare Massimo e Minimo}
Una volta trovato un punto stazionario $(x,y)$, voglio capire se si tratta di un punto di Massimo o di Minimo:
\definizione{
    Condizione \textbf{sufficiente} affinché $(x, y)$ sia un punto di \textbf{Minimo} per $f$ è che:
    \begin{itemize}
        \item $\nabla f(x,\, y) = 0$ (derivate parziali tutte nulle)
        \item $H_f (x, y)$ sia \textbf{definita positiva}
    \end{itemize}
    Condizione \textbf{sufficiente} affinché (x, y) sia un punto di \textbf{Massimo} per $f$ è che:
    \begin{itemize}
        \item $\nabla f(x, y) = 0$ (derivate parziali tutte nulle)
        \item $H_f (x, y)$ sia \textbf{definita negativa}
    \end{itemize}
}



\subsection{Matrice Definita Positiva/Negativa}
Il Concetto di Matrice Definita Positiva/Negativa ci serve per distinguere i punti di Massimo, Minimo e di Sella.
\definizione{
    Una Matrice Quadrata si Dice:
    \begin{itemize}
        \item Definita Positiva se tutti gli Autovalori sono $>0$
        \item Semi Definita Positiva se tutti gli Autovalori sono $\geq 0$
        \item Definita Negativa se tutti gli Autovalori sono $<0$
        \item Semi Definita Negativa se tutti gli Autovalori sono $\leq0$
    \end{itemize}
}

\paragraph{Osservazione}
Sia $f(x,\, y)$ una funzione derivabile due volte, allora:
\begin{itemize}
    \item $f$ è convessa nell'insieme A se e solo se $H_f (x,\, y)$ è semi definita positiva.
    \item $f$ è concava nell'insieme A se e solo se $H_f (x,\, y)$ è semi definita negativa.
\end{itemize}


\section{Algoritmi Numerici per i PNL Multivariati}
Come per la PNL Monovariata, anche per la Multivariata esistono degli algoritmi numerici per trovare l'ottimo qualora non fosse possibile trovarlo analiticamente.
\\Gli algoritmi che affronteremo sono:
\begin{itemize}
    \item Metodo del Gradiente (Steepest Descent)
    \item Metodo di Newton
\end{itemize}
Entrambi gli algoritmi utilizzano strategie di tipo \textbf{Line Search}.
\subsection{Gli algorimti Line Search}
Questo tipo di algoritmo genera una succesione di punti $x_k \in \R^n$ in modo che il punto $x_{k+1}$ sia ottenuto a partire dal punto $x_k$ muovendosi lungo una \textbf{Direzione di Salita (Massimo) o di Discesa (Minimo)}.
\definizione{
    Dati $f:\R^n \to \R$, $x\in \R^n$ con $f$ Derivabile in $x$.
    \\Definiamo:
    \begin{itemize}
        \item Direzione di Discesa un generico vettore $v\in \R^n$ tale che $\left\langle v, f(x)\right\rangle < 0$
        \item Direzione di Salita un generico vettore $v\in \R^n$ tale che $\left\langle v, f(x)\right\rangle > 0$
    \end{itemize} 
    Dove $\left\langle \right\rangle $ indica il Prodotto scalare tra vettori.
}

\subsection{Schema generico per il Line Search}
Dati $f:\R^n \to \R$, $x\in \R^n$ con $f$ Derivabile in $x$.
\\Se vogliamo cercare un generico punto di minimo di una funzione, possiamo usare la seguente strategia:
\begin{enumerate}
    \item Poniamo $k=0$ e consideriamo un generico punto $x^k$
    \item Determiniamo una Direzione di Discesa $d^k$
    \item Cerchiamo un nuovo punto $x^{k+1}$ lungo la direzione $d^k$:
    \[x^{k+1} = x^k \pm \alpha^k \cdot d^k\]
    dove \begin{itemize}
        \item $k$ è la generica iterazione
        \item $\alpha^k$ è una quantità scalare $ > 0$ chiamata step-size.
    \end{itemize}
\end{enumerate}

\paragraph{Step-Size} Lo Step-Size è il valore che determina di quanto ci andremo a spostare nella direzione $d_k$.
Esso è inversamente proporzionale al numero di iterazioni.
\\Lo step-size $\alpha^k$ corrisponde a:
\[
    \text{max/min } f(x^k \pm \alpha^k \cdot d^k)
\]
e uso un metodo di Max/Min per trovare $\alpha^k$ ottimale, per esempio con una soluzione analitica:
\[ 
    \frac{\delta g(\alpha^{k} )}{\delta \alpha} = 0 \text{ dove } 
    g(\alpha) = f(x^k \pm \alpha^k \cdot d^k)
\]

\subparagraph{Ovvero} Trovato un $x^{k+1}$ che ha $\alpha$ come incognita,
lo uso per trovare lo step-size ponendo $g(\alpha) = f(x^{k+1})$ e $\alpha^* = \text{max } g(\alpha)$.
\\$ max g(\alpha)$ lo posso spesso risolvere analiticamente, ponendo $g'(\alpha) = 0$.

\section{Metodo del Gradiente}
Il metodo del gradiente è un algoritmo Line Search e come direzione di crescita $d^k$ usa:
\begin{itemize}
    \item $d^k= \nabla f(x^k)$ per i problemi di Massimo
    \item $d^k= - \nabla f(x^k)$ per i problemi di Minimo
\end{itemize}
E come formula generale per il punto $x^{k+1}$:
\[x^{k+1} = x^k \pm \alpha^k \cdot \nabla f(x^k)\]
\nb{$\pm \nabla f(x^k)$ è la direzione di crescita, quindi il $\pm$ diventa $+$ se è un problema di Massimo, e $-$ se è un problema di minimo }

\subsection{L'algoritmo}
\begin{enumerate}
    \item Pongo $k=0$ e considero un generico $x^k$
    \item Calcolo $\nabla f(x^k)$ e pongo la direzione di crescita $d^k=\pm \nabla f(x^k)$
    \item Cerco un nuovo punto $x^{k+1}$ lungo la direzione $d^k$:
    \[x^{k+1} = x^k \pm \alpha^k \cdot d^k\]
    \item Calcolo $\alpha^k$ come la soluzione di $\frac{df(x^k \pm \alpha^k \cdot d^k)}{d\alpha^k} = 0$
    
\end{enumerate}

\paragraph{Criteri di Arresto}
\begin{itemize}
    \item $|f(x^{k+1}) - f(x^k)| < \epsilon_1$ mi fermo.
    \item $||f(x^{k+1}) || < \epsilon_2$ mi fermo, dove $|| f ||$ è la norma di $f$.
\end{itemize}

\paragraph{Osservazioni} Noto che l'algoritmo del gradiente si differenzia da un generico Line Search solo dalla determinazione della direzione di Crescita.
Inoltre noto che se $\nexists\; \alpha^k > 0 \implies \nexists$ una direzione di Salita.

\section{Metodo di Newton}
A differenza del metodo del Gradiente, il metodo di Newton ($n$-dimensionale) utilizza un'approssimazione quadratica di $f(x)$ per ottenere un nuovo punto.

\paragraph{Sviluppo di Taylor} per funzioni a più variabili:
\[
  f(x^k+ \Delta x) = f(x^k) + \nabla f(x^k) \cdot \Delta x + \frac{1}{2} \Delta x \cdot H f(x^k) \Delta x   
\]
In questo caso $x^k+ \Delta x = x^{k+1}$
\paragraph{Direzione di Miglioramento}
Essendo un algoritmo Line Search, Newton segue una direzione di miglioramento per il nuovo punto $(x^k+ \Delta x)$ così definita:
\[
    \frac{\delta f(x^k+ \Delta x)}{\delta \Delta x} = 0 \to H(x^k) \Delta x = -\nabla f(x^k)
\]

$\Delta x$ è il vettore spostamento, e in questo caso è l'incognita.

\subparagraph{Osservazione} Newton si muove ad un punto stazionario del II ordine dello sviluppo in serie di Taylor della funzione.

\subsection{l'Algoritmo}
Tenendo peresente che $\Delta x = x^{k+1} - x^k$
\begin{enumerate}
    \item Pongo $k=0$ e considero un generico $x^k$
    \item Calcolo $\nabla f(x^k)$ e $H(x^k)^{-1}$
    \item Cerco un nuovo punto $x^{k+1}$:
    \[x^{k+1} = x^k - H(x^k)^{-1} \cdot \nabla f(x^k)\]
\end{enumerate}

\subparagraph{Criteri di Arresto}
Sono gli stessi del metodo del gradiente.
\begin{itemize}
    \item $|f(x^{k+1}) - f(x^k)| < \epsilon_1$ mi fermo.
    \item $||f(x^{k+1}) || < \epsilon_2$ mi fermo, dove $|| f ||$ è la norma di $f$.
\end{itemize}

\subsection{Evitare il calcolo della Matrice Inversa}
Osservando la formula per il calcolo del nuovo punto:
\[ x^{k+1} = x^k - H(x^k)^{-1} \cdot \nabla f(x^k)\]
Notiamo che ci richiede di calcolare $-H_f(x_k,y_k)^{-1}$, ovvero la \emph{Matrice inversa dell'hessiana} nel punto $(x_k,y_k)$.
\\Calcolare una matrice inversa è \textbf{molto oneroso dal punto di vista computazionale}, esiste però un modo per evitare di calcolarlo.
\paragraph{Considerazione}
Nel calcolo del nuovo punto, $[-H(x^k)^{-1} \cdot \nabla f(x^k)]$ equivale al \emph{vettore spostamento $\Delta x$} (scritto anche come $(v_1,v_2)^T$), di conseguenza:
\[
    - H(x^k)^{-1} \cdot \nabla f(x^k)=\begin{pmatrix} v_1 \\ v_2 \end{pmatrix} \implies H_f(x_k,y_k) \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} = - \nabla f(x_k,y_k) 
\]
In questo modo per poter trovare $\Delta x$ dobbiamo soltanto risolvere un sistema di equazioni invece di calcolare una matrice inversa,
il che è molto meno oneroso computazionalmente.


\subsection{Newton VS Gradiente}
\begin{itemize}
    \item Newton utilizza sia Gradiente che Hessiana ad ogni iterazione.
    \item Newton se converge, converge molto più rapidamente del metodo del gradiente ma richiede uno sforzo computazionale molto maggiore.
    \item Se la funzione è quadratica, Newton converge ad una sola iterazione.
    \item Nel caso di funzioni più complesse, la matrice Hessiana risulta più complessa da calcolare.

\end{itemize}


