\chapter{PNL Multivariata}
Nella PNL Monovariata utilizziamo le informazioni di Derivata Prima e Seconda per trovare l'ottimo.
Anche nel caso della PNL Multivariata usiamo gli stessi concetti, bisogna peró estenderli agli spazi multidimensionali.
\\Vogliamo estendere i concetti di derivata (prima e seconda) visti per funzioni in una variabile, al caso di funzioni in più variabili.
\\In particolare, consideriamo uno spazio $\R^n$, ovvero uno spazio vettoriale a $n$ dimensioni.
Se nel caso $\R^1$ avevamo Derivata I e II, nel caso $n$-dimensionale:
\begin{itemize}
    \item $f'(x) \to$ Gradiente.
    \item $f''(x) \to$ Hessiana.
\end{itemize} 

\section{Derivate Parziali}

Per conoscere Gradiente ed Hessiana dobbiamo introdurre il concetto di Derivata Parziale.

\definizione{
    Le \textbf{Derivate Parziali} (anche dette derivate direzionali) sono le derivate in un iperspazio per una determinata direzione $v$.
    \paragraph{Quindi} se ho $f(x,y,z)$ posso ottenere
    \[
        \frac{d f(x,y,z)}{dx}, \frac{d f(x,y,z)}{dy}, \frac{d f(x,y,z)}{dz}
    \]
    Ovvero le derivate parziali di $f(x,y,z)$ in ognuna delle sue variabili.
    \linebreak \small{
        \nb{Se derivo $f(x,y,z)$ in $x$, allora $y$ e $z$ vengono trattate come costanti nel processo di derivazione.}
    }
}

\section{Gradiente ed Hessiana}
Ora possiamo quindi introdurre Gradiente ed Hessiana:
\subsection{Gradiente}
Il gradiente di $f(x, y)$ è per definizione il vettore $\text{D}f$ le cui componenti sono le \textbf{derivate parziali} di $f$, ovvero
$$\nabla f(x,\, y) = \left[\frac{\partial f(x,\, y)}{\partial x},\, \frac{\partial f(x,\, y)}{\partial y}\right]^T$$
\\Anche in questo caso esistono varie notazioni per indicare il gradiente: $$\nabla f,\quad \text{D}f(x,\, y),\quad \nabla f(x,\, y),\quad \text{grad} f$$

\subsection{Hessiana}
L'insieme delle derivate seconde costituisce una matrice quadrata chiamata matrice Hessiana, e si denota con il simbolo $\text{D}^2 f$ o $H_f$.
$$H_f f(x,\, y) = \left[\begin{matrix}
    \frac{\partial^2 f(x,\, y)}{\partial x \partial x} & \frac{\partial^2 f(x,\, y)}{\partial x \partial y} \\
    \frac{\partial^2 f(x,\, y)}{\partial y \partial x} & \frac{\partial^2 f(x,\, y)}{\partial y \partial y}
\end{matrix}\right]$$

In cui ogni riga della matrice Hessiana "corrisponde" ad un elemento del Gradiente, 
mentre ogni colonna corrisponde alla derivata parziale in ognuna delle variabili di $f$.
\paragraph*{Derivate Pure}
Gli elementi sulla diagonale principale vengono chiamati \textbf{derivate pure}, per distinguerli dagli altri elementi chiamati \textbf{derivate miste}.


\section{Metodo Analitico per l'Ottimo}
Il gradiente riassume le informazioni di crescita della funzione lungo tutte le direzioni. I punti per cui il gradiente
si annulla ($\nabla f = 0$), prendono il nome di \textbf{punti stazionari}.

\subsection{Punti stazionari n-dimensionali}
\definizione{
    Se abbiamo un punto $(x_0,\, y_0)$ t.c. 
    \[
        \nabla f(x_0,\, y_0) = \left[\frac{\partial f(x,\, y)}{\partial x},\, \frac{\partial f(x,\, y)}{\partial y}\right]^T = [0,\, 0]^T
    \]
    Allora questo é un \textbf{punto stazionario}, e potrebbe essere un punto di massimo, minimo o di sella.
}

\paragraph*{Punto di Sella}
Se nel caso 1D ogni punto in cui la derivata si annulla é un punto di Max o Min, nel caso n-dimensionale puó anche
essere un \emph{punto di sella}, ovvero un punto che é massimo in una direzione (variabile) e minimo nell'altra.

\subsection{Trovare Massimo e Minimo}
Una volta trovato un punto stazionario $(x,y)$, voglio capire se si tratta di un punto di Massimo o di Minimo:
\definizione{
    Condizione \textbf{sufficiente} affinché $(x, y)$ sia un punto di \textbf{Minimo} per $f$ è che:
    \begin{itemize}
        \item $\nabla f(x,\, y) = 0$ (derivate parziali tutte nulle)
        \item $H_f (x, y)$ sia \textbf{definita positiva}
    \end{itemize}
    Condizione \textbf{sufficiente} affinché (x, y) sia un punto di \textbf{Massimo} per $f$ è che:
    \begin{itemize}
        \item $\nabla f(x, y) = 0$ (derivate parziali tutte nulle)
        \item $H_f (x, y)$ sia \textbf{definita negativa}
    \end{itemize}
}



\subsection{Matrice Definita Positiva/Negativa}
Il Concetto di Matrice Definita Positiva/Negativa ci serve per distinguere i punti di Massimo, Minimo e di Sella.
\definizione{
    Una Matrice Quadrata si Dice:
    \begin{itemize}
        \item Definita Positiva se tutti gli Autovalori sono $>0$
        \item Semi Definita Positiva se tutti gli Autovalori sono $\geq 0$
        \item Definita Negativa se tutti gli Autovalori sono $<0$
        \item Semi Definita Negativa se tutti gli Autovalori sono $\leq0$
    \end{itemize}
}

\paragraph{Osservazione}
Sia $f(x,\, y)$ una funzione derivabile due volte, allora:
\begin{itemize}
    \item $f$ è convessa nell'insieme A se e solo se $H_f (x,\, y)$ è semi definita positiva.
    \item $f$ è concava nell'insieme A se e solo se $H_f (x,\, y)$ è semi definita negativa.
\end{itemize}


\subsection{Gli Autovalori di una Matrice}
L'Autovalore di una matrice quadrata $A$ é un valore scalare $\lambda$ t.c.:
\[
    \exists v \neq 0 \text{ t.c. } Av = \lambda v    
\]


$v$ é un vettore colonna non nullo, ed é l'autovettore per la matrice $A$ relativo a $\lambda$ t.c.
\[
    Av - \lambda v = (A \cdot \lambda I) \cdot v
\]
Con $I$ Matrice Identitá di A.




