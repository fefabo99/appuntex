\chapter{Ottimizzazione Non Lineare Vincolata}
Consideriamo un generico problema di Ottimizzazione Non Lineare Vincolata:

\begin{center}
    opt $f(x)$
    \\s.a. $g_i(x) \{ =/\leq/\geq\} l$ con i= $1,...,m$
\end{center}
In questo caso \emph{Non esistono algoritmi di risoluzione}, ma ci sono diversi approcci per semplificare
e risolvere il problema.

\paragraph{Gli approcci} che affronteremo noi sono:
\begin{itemize}
    \item Dimensionality Reduction
    \item Moltiplicatori di Lagrange
    \item Condizioni di Karush-Kuhn-Tucker
\end{itemize}

\section{Dimensionality Reduction}
L'approccio del Dimensionality Reduction (o riduzione del numero delle variabili libere) consiste nell'utilizzare
i vincoli di uguaglianza del problema per ridurre il problema a un PNL Monovariato non vincolato.

\subsection{Facciamo un Esempio}
Dato il problema:
\begin{center}
    min $(x_1 - 2)^2 + 2(x_2-1)^2$
    \\s.a. $x_1+4x_2 =3$
\end{center}
Possiamo Esplicitare $x_1$ nel vincolo:
\[
    x_1+4x_2 =3 \to x_1 = 3 - 4x_2  
\]
Per poi sostituirlo nella funzione obiettivo:
\[
    \text{min }(x_1 - 2)^2 + 2(x_2-1)^2 \to  \text{min }(3 - 4x_2 - 2)^2 + 2(x_2-1)^2
\] 
In modo da ottenere un problema di ottimizzazione in una variabile, che possiamo risolvere analiticamente:
\[ \text{min }(3 - 4x_2 - 2)^2 + 2(x_2-1)^2 \implies x_2=\frac{1}{3}\]
Da cui, con la funzione obiettivo originale troviamo $x_1=\frac{5}{3}$.
In questo modo otteniamo la soluzione $(\frac{5}{3}, \frac{1}{3})$.

\subsection{É un metodo generalizzabile?}
Non sempre il metodo del Dimensionality Reduction é generalizzabile. Quando é possibile?
\\Supponiamo di avere un problema di $opt$ soggetto a $l$ vincoli di \emph{Uguaglianza}.
\textbf{SE} é possibile esplicitare $l$ variabili in funzione delle restanti $n-l$ variabili utilizzando i vincoli
di uguaglianza del problema, allora possiamo trasformare tale problema in un problema di $opt$ \emph{non vincolata} con $n-l$ variabili.

\begin{center}
    opt $f(x_1,...,x_n)$
    \\s.a. $g_j(x_1,...,x_n) =b_j$ con $j=1,...,l$
    \\$\Downarrow$
    \\opt $f(x_1,...,x_{n-l})$
    \\$x_{n-l+1} = g_1(x_1,...,x_{n-l})$
    \\ $\vdots$ 
    \\$x_{n} = g_l(x_1,...,x_{n-l})$
\end{center}
Questa condizione peró molto spesso NON é rispettata, quindi il Dimensionality Reduction non é sempre utilizzabile.

\section{Il metodo dei Moltiplicatori di Lagrange}

\subsection{La Funzione Lagrangiana}
Per capire il metodo dei moltiplicatori di Lagrange, bisogna prima introdurre il concetto
di \emph{Funzione Lagrangiana}:
\definizione{
    Consideriamo un generico problema di PNL Vincolata con \emph{solo vincoli di uguaglianza}:
    \begin{center}
        opt $f(x_1,...,x_n)$
        \\s.a. $g_j(x_1,...,x_n) =0$ con $i=1,...,m$
    \end{center}
    La seguente funzione in $n+m$ variabili prende il nome di \textbf{Funzione Lagrangiana} del problema:
    \[
        L(x_1,...,x_n,\lambda_1,...,\lambda_m) = f(x_1,...,x_n) + \sum_{i=0}^{m} \lambda_i \cdot g_i(x_1,...,x_n)
    \]
    Le variabili $\lambda_j$, di cui ce ne sono tante quante i vincoli, prendono il nome di \textbf{Moltiplicatori di Lagrange}.
}

\paragraph{I punti stazionari della Lagrangiana}
Come mai ci interessa la funzione Lagrangiana?
I punti stazionari della Lagrangiana (Ovvero i punti in cui il gradiente della lagrangiana si azzera)
sono \emph{Fortemente legati ai punti di massimo e minimo della funzione}.

\subsection{Il Metodo}
\definizione{
    Consideriamo un generico problema di ottimizzazione non lineare del tipo:
    \begin{center}
        opt $f(x_1,...,x_n)$
        \\s.a. $g_j(x_1,...,x_n) =0$ con $i=1,...,m$
    \end{center}
    sia $x^* = (x^*_1,...,x^*_n)$ il punto stazionario di $f$, allora esistono $m$
    moltiplicatori di Lagrange $\lambda^* = (\lambda^*_1,...,\lambda^*_m)$
    tali che $(x^*,\lambda^*)$ è un punto stazionario della Lagrangiana Associata.
    \[
        L(x,\lambda) = f(x) + \sum_{i=1}^{m} \lambda_i \cdot g_i(x)
        \]
    \paragraph{Osservazioni}
    \begin{itemize}
        \item I punti $x^*$ e $(x^*,\lambda^*)$ possono essere punti stazionari \emph{di tipo diverso}.
        \item Tale condizione fornisce una \textbf{Condizione Necessaria ma Non Sufficiente} per l'ottimizzazione del problema vincolato.
        
    \end{itemize}
}