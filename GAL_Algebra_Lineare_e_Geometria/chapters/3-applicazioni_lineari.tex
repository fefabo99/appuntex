\chapter{Applicazioni Lineari}

\section{Introduzione}
Le funzioni insiemistiche normali non sono adatte a studiare gli spazi vettoriali,
quindi per studiare gli spazi vettoriali tramite funzioni è meglio imporre condizioni a tali funzioni.

\definizione{
	Siano $V$ e $W$ due spazi vettoriali.
	\\ Un'applicazione $f:V\to W$ è \textbf{Lineare}, o un \textbf{Omomorfismo} se:
	\begin{enumerate}
		\item $f(\underline{v}_1 + \underline{v}_2) = f(\underline{v}_1) + f(\underline{v}_2)$
		\item $f(\lambda \underline{v}) = \lambda f(\underline{v})$.
	\end{enumerate}
	$\forall \underline{v}, \underline{v_1}, \underline{v_2} \in V,\lambda \in \R$
}
Equivalentemente l'immagine di una combinazione lineare è la combinazione lineare delle immagini:
$f(\lambda\underline{v}_1 + \beta\underline{v}_2) = \lambda f(\underline{v}_1) + \beta f(\underline{v}_2)$
\\
\\Se $V=W$, allora l'applicazione lineare si chiama \textbf{Endomorfismo} (di $V$).
\paragraph{Corollari}
Prendiamo sempre $f$, $V$ e $W$ dalla definizione:
\begin{itemize}
	\item \textbf{Zero}: $f(\underline{0}_v) = \underline{0}_w$
	\item \textbf{Opposto}: $f(-\underline{v}) = -f\underline{v}$.
	\item \textbf{Sottospazi}: Se $U<V$, $f(U) < W$
	    %Da completare
\end{itemize}

\osservazione{Se $\{\underline{v_i}\}$ è una base di $V$, ogni scelta di $f(\underline{v_i})$ è compatibile con le condizioni di linearità.
	\\Cioè, per ogni scetla di $\{\underline{w_i}\} \subset W \exists! f:V\to W$ Lineare t.c. $f(\{\underline{v}_i\}) = \underline{w}_i$
	%Tale $f$ dece essere definita come ...
}

\paragraph{Corollario}
Sia $f:V\to W$ lineare, allora dim $f(v) \geq$ dim $V$.

\subsection{Isomorfismi}
\definizione{
	\begin{enumerate}
		\item Due spazi vettoriali $V$ e $W$ sono detti isomorfi se esistono:
		      \begin{center}
			      $f:V\to W$ e $g:W\to V$
		      \end{center}
		      diverse e entrambe lineari t.c:
		      \begin{center}
			      $g\circ f = id_V$ e $f\circ g = id_W$
		      \end{center}
		      Due spazi sono isomorfi sse $dim(V) = dim(W)$.
		\item $f:V\to W$ è detta \textbf{isomorfismo} se è lineare e iniettiva.
	\end{enumerate}
}

\subsection{Sottospazi}
Sia $f:V \to W$ lineare. Allora, $f^{-1}(\underline{z}) < V$?
Sappiamo che per essere un sottospazio deve contenere l'identità di $V$, quindi
$0_v \in ff^{-1}(\underline{z})$, e quindi $\underline{z} = 0_w$ per la proprietà 1 del corollario.
Si verifica che $f^{-1}0_w <V$.

\definizione{
Il \textbf{Kernel} (o Nucleo) di un'applicazione lineare é un sottoinsieme del suo dominio formato da 
\emph{tutti e soli i vettori che hanno come immagine lo zero del codominio}.	

%Prende il nome di \textbf{nucleo} (o kernel) di un'applicazione lineare un particolare sottoinsieme del dominio dell'applicazione,
%formato da tutti e soli i vettori del dominio che hanno come immagine lo zero del codominio.
\[ Ker(F) := \{\underline{v}\in V : F(\underline{v}) = \underline{0}_W\}\]
}
%\definizione{ $f^{-1}(\underline{o}_w)$ si chiama \textbf{nucleo}, o kernel, di $f$. e si indica con $N(f)$.}
Il Kernel $Ker(f)$ è importante per il seguente teorema:
\teorema[fondamentale dell'isomorfismo]{Sia $f:V\to W$ lineare. Allora $f$ può essere fattorializzata come:
	DA AGGIUNGERE IMMAGINE
}

\subsection{Teorema di Grasman}
\teorema[di Grasman]{$f:V\to W$ lineare. Allora $dim(V)=dim(f(V))=dim(N(f))$}
\paragraph{corollario} sia $V_1,V_2<V$, allora $dim(V_1+V_2)=dim(V_1) + dim(V_2) - dim(V_1 \cap V_2)$

\paragraph{Corollario} sia $dim(W)=dim(V), f:V\to W$, allora:
\begin{center}
	$f$ iniettiva $\leftrightarrow$ $f$ è suriettiva $\leftrightarrow$ $f$ è Biiettiva
\end{center}

\section{Matrice Associata}
%% Da youmath
Una matrice assocaita a un'applicazione lineare \emph{rappresenta la trasformazione lineare cui è riferita rispetto a due fissate basi degli spazi vettoriali} di partenza e d'arrivo.
\\Tale matrice fornisce tutte le informazioni relative alla trasformazione cui è associata, dunque riveste un ruolo da protagonista nell'ambito delle applicazioni lineari.

\subsection{Costruzione di una Matrice Associata}
Consideriamo due spazi vettoriali $V,W$ definiti sullo stesso campo e di dimensioni rispettivamente pari a $n,m$.
e consideriamo un'applicazione lineare $f:V\to W$.
\\Siano:
\begin{center}
	$B_V = \{v_1,...,v_n\}$ e $B_W=\{w_1,...,w_m\}$ due basi, di $V$ e $W$
\end{center}
Per \textbf{costruire} la matrice associata all'applicazione $f$ rispetto alle basi $B_V$ e $B_W$ occorre procdere nel modo seguente:
\begin{enumerate}
	\item Determinare l'immagine rispetto ad $f$ di ogni vettore $v_i \in B_V$:
	      \[ f(v_1),...,f(v_n)\]
	\item I vettori ottenuti sono elementi di $W$, e possono essere espressi come combinazione linerae dei vettori di $B_W$:
	\begin{center}
        $f(v_1) = a_{11}w_1 + ... + a_{m1}w_m $
        \\ $f(v_2) = a_{12}w_1 + ... + a_{m2}w_m $
        \\$ \vdots$
        \\$f(v_n) = a_{11n}w_1 + ... + a_{mn}w_m $
    \end{center}
    \item La matrice che ha per $j-$esima colonna il vettore delle coordinate dell'immagine $f(v_j)$ rispetto alla abse di $W$ si dice matrice associata all'applicazione lineare $f$ rispetto alle basi $B_V$ e $B_W$:
    \[ M_{B_V}^{B_W}			 = 
        \begin{pmatrix}
            a_{11} & a_{12} & ... & a_{1n} \\
            a_{21} & a_{22} & ... & a_{2n} \\
            \vdots & \vdots & ... & \vdots \\
            a_{m1} & a_{m2} & ... & a_{mn} \\
        \end{pmatrix}
    \]
\end{enumerate}

\section{Prodotti Interni} %Non so se va qui
I prodotti interni sono una struttura supplementare agli spazi vettoriali.
\definizione{Un prodotto interno in uno spazio vettoriale $V$ è una funzione $V\times V \to K$:
\[ (v_1,v_2) \to <\underline{v_1}, \underline{v_2}> \]
}
Tale che:
\begin{enumerate}
	\item $<\underline{v},\underline{w}> = <\underline{w},\underline{v}>$
	\item $<\alpha \underline{v_1} + \beta \underline{v_2},\underline{w}> =\alpha <\underline{v_1},\underline{w}> + \beta <\underline{v_2},\underline{w}> $
	\item $<\underline{v},\underline{v}> > 0$ se $\underline{0}\neq \underline{v}$
\end{enumerate}