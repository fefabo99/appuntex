\chapter{Programmazione non Lineare}
Un \textbf{problema di programmazione non lineare (PLN)} può essere formulato come: $$\text{opt}\, \mathit{f}(\mathbf{x})$$ soggetto ai seguenti vincoli: $$g_j(\mathbf{x})\, \le\, 0\quad \text{con}\quad j\, =\, 1, ..., m$$ $$x_i\, \ge\, 0\quad \text{con}\quad i\, =\, 1, ..., n$$ $\text{dove opt} = 
\begin{cases} 
        min \\ 
        max
\end{cases}
$
\\in cui $f\text{:}\; \R^n \to \R\,$ e $\, g_j\,\text{:}\; \R^n \to \R\,$ sono funzioni note di $x\, \in\, \R^N$.

\section{Tipologie di Programmazione non-lineare}
A seconda delle caratteristiche del problema abbiamo diversi tipi di programmazione non lineare.

\subsubsection{Ottimizzazione non vincolata}
I problemi di ottimizzazione non vincolata non hanno vincoli sulla regione ammissibile. Quindi l'obiettivo è semplicemente $$\text{max}\, f(\mathbf{x})\quad\, o\quad\, \text{min}\, f(\mathbf{x})\quad\, x\in\R^n$$.

\subsubsection{Ottimizzazione con vincoli lineari}
I problemi di ottimizzazione con vincoli lineari sono caratterizzati da tutte le funzioni $g_i(\mathbf{x})$ lineari, ma dalla funzione obiettivo non-lineare. Sono problemi che vanno a dare origine ad una \textit{regione ammissibile convessa}. Un caso particolare è la \textit{programmazione quadratica}, in cui la funzione obiettivo è una funzione quadratica.

\subsubsection{Ottimizzazione convessa}
Problemi in cui $f(\mathbf{x})$ è una funzione \textbf{concava} o \textbf{convessa} e ogni funzione $g_i(\mathbf{x})$ è \textbf{convessa}.

\subsubsection{Ottimizzazione non convessa}
I problemi di programmazione non convessa comprendono tutti i problemi che non soddisfano le ipotesi di convessità. Sono più difficili da risolvere perché possono presentare diversi punti di minimo/massimo.

\section{Massimizzazione di funzioni concave}
Consideriamo un problema di massimizzazione con una funzione obiettivo $f(x)$ concava da massimizzare.
Essendo questa funzione \textbf{concava}, sappiamo che una condizione sufficiente affinchè $x^*$ sia \textbf{punto di massimo} è che:
\[
    \frac{d}{dx}f(x^*) = 0 \text{, ovvero il punto di massimo è dove la derivata è stazionaria}
\]
Capiamo qundi che se \emph{una equazione può essere risolta analiticamente} allora il procedimento per trovare l'ottimo termina.
\\Un discorso equivalente si può fare per problemi di minimizzazione di funzioni convesse.
\subsection{Algoritmi per la risoluzione numerica}
E se non posso risolverla analiticamente?
In mancanza di una risoluzione analitica (per esempio per una funzione troppo complicata) sono disponibili algoritmi per la risoluzione numerica del problema.

\subparagraph{L'idea} degli algoritmi per la risoluzione numerica:
Si costruisce una sequenza di punti $\{x_k\}$ t.c.: (nei casi di minimizzazione)
\[\lim_{x\to +\infty} x_k = x^* \text{ e } f(x_{k+1}) \leq f(x_k)\]
Ad ogni iterazione $k$, partendo da $x_k$ si esegue una ricerca sistematica per identificare un punto migliore $x_k$.

\paragraph{I criteri di Arresto}
A differenza dell'algoritmo del simplesso, in questo caso la sequenza di punti $\{x_k\}$ non è detto che converga alla soluzione ottima del problema
in un numero finito di iterazioni.

Quindi quando fermo la sequenza di punti?
\begin{itemize}
    \item La soluzione è sufficientemente accurata, ovvero $\frac{df(x_k)}{dx} \simeq 0$
    \item Quando si è raggiunto un numero massimo di iterazioni $N$ o un tempo computazionale massimo.
    \item I progressi sono lenti, ovvero $|x_{k+1} - x_k| < \epsilon_x$ o  $|f(x_{k+1}) - f(x_k)| < \epsilon_f$
    \item La soluzione diverge
    \item Si verificano Cicli.
\end{itemize}

\subsection{Gli algoritmi}
Esistono due tipi di algoritmi:
\\\textbf{Dicotomici}: Algoritmi di ricerca per individuare un determinato valroe (per il quale la funzione derivata si annulla) all'interno di un intervallo che ad ogni iterazione viene ridotto.
\\\textbf{ Di approssimazione}: utlizzando approsimanzioni locali della funzione

Per l'ottimizzazione di funzioni in una variabile tratteremo:
\begin{itemize}
    \item Metodo di Bisezione (Dicotomico)
    \item Metodo di Newton (Di Approssimazione)
\end{itemize}

\section{Il metodo di Bisezione}
L'idea di questo algoritmo è:
Se $f(x)$ è \emph{continua e concava} in un intervallo chiuso $[a.b]$ allora, considerando \textbf{un generico punto $x_k$}, se:
\begin{itemize}
    \item $\frac{d}{dx} f(x_k) < 0 \implies $ l'ottimo $x^*$ si trova a \textbf{sinistra} di $x_k$.
    \item $\frac{d}{dx} f(x_k) > 0 \implies $ l'ottimo $x^*$ si trova a \textbf{destra} di $x_k$.
    \item $\frac{d}{dx} f(x_k) \simeq 0 \implies $ $x_k \simeq x^*$, ovvero è circa l'ottimo.
\end{itemize}

\paragraph*{Il Metodo}
Il metodo di bisezione è una procedura abbastanza intuitiva.
Guardando la pendenza della derivata per un punto $x_k$ in una funzione \emph{concava e derivabile:}
\begin{itemize}
    \item Se $f'(x_k) > 0$:
    \begin{itemize}
        \item Il nuovo punto $x_{k+1}$ lo ottengo spostandomi verso Destra
        \item $x_k$ rappresenta un \emph{Estremo Inferiore} $\underline{x}$ per il punto $x^*$
    \end{itemize}
    \item Se $f'(x_k) < 0$:
    \begin{itemize}
        \item Il nuovo punto $x_{k+1}$ lo ottengo spostandomi verso Sinistra
        \item $x_k$ rappresenta un \emph{Estremo Superiore} $\bar{x}$ per il punto $x^*$
    \end{itemize}
\end{itemize}

Ad ogni iterazione $k$ posso identificare un sotto-intervallo di ricerca $[a_k, b_k] \subset [a.b]$
per ridurre lo spazio di ricerca in modo da identificare un'iterazione $k$ per cui $|b_k - a_k| < 2\epsilon$ 
con $|a_k - x^*| < \epsilon$ e $|x^*-b_k|<\epsilon$

\subsection{L'algoritmo}
\paragraph*{Inizializzazione}
$k=0$ e $\epsilon$ piccolo a piacere.
\\Si determinano i valori $\underline{x}$ e $\bar{x}$ per cui la derivata sia rispettivamente positiva e negativa.
si seleziona il punto iniziale $x_0 = \frac{\underline{x} + \bar{x}}{2}$

\paragraph*{Iterazione del metodo di bisezione} bisogna: \\
\begin{enumerate}
    \item Calcolare $f'(x_k)$
    \item 
\begin{lstlisting}[mathescape=true]
if $f'(x)=0$ then $x_k = x^*$
else
    if $f'(x)<0$
        $\bar{x} = x_k$
    else
        $\underline{x} = x_k$
\end{lstlisting}
    \item Pongo $x_{k+1} = \frac{\underline{x} + \bar{x}}{2}$ e $k=k+1$ 
\end{enumerate} 

Criterio di arresto: se $underline{x} + \bar{x} \leq 2\epsilon$ allora il punto di ottimo $x^*$ avrà una distanza minore di $\epsilon$ da uno dei due. 

\section*{il Metodo di Newton}
Il metodo di newton è molto simile a quello di bisezione ma  considera anche la derivata seconda e non solo l'informazione relativa alla derivata prima.

\paragraph*{L'approssimazione quadratica}
il Metodo di newton ottiene un'approssimazione quadratica (in $x^2$) tramite la formula di Taylor centrata in $x_k$
\[
    f(x_k) \sim f(x_k) + f'(x_k)(x_{k+1} - x_k) + \frac{1}{2} f''(x_k)(x_{k+1} - x_k)^2
\]
L'approsimazione quadratica è in funzione solo di $x_{k+1}$!
Calcoliamo la derivata dell'approssimazione quadratica
\[
    f'(x_k)+f''(x_k)(x_{k+1} - x_k)= 0 \to x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}
\]

Quindi, l'idea dell'algoritmo di Newton è quella di usare l'ottimo dell'approsimazione quadratica di $f(x)$ dato da:
\[
    x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}
\]
Questa formula viene usata ad ogni generiaca iterazione $k$ per calcolare il punto successivo $x_{k+1}$

\paragraph*{Importante}
Se $f(x)$ è concava allora $x_k$ converge verso un punto di massimo, perchè?
\begin{itemize}
    \item $x_k$ è a Sinistra del punto di massimo
    \\ $\implies f'(x_k) > 0 \implies -\frac{f'(x_k)}{f''(x_k)}>0 \implies x_{k+1}> x_k$
    \item $x_k$ è a Destra del punto di massimo
    \\ $\implies f'(x_k) < 0 \implies -\frac{f'(x_k)}{f''(x_k)}<0 \implies x_{k+1}< x_k$
\end{itemize}

\subsection{L'algoritmo di newton}
\begin{itemize}
    \item Inizializzazione
    \begin{itemize}
        \item si fissa $\epsilon$ e $k=0$ 
    \end{itemize} 
    \item Iterazione del metodo di newton
    \begin{itemize}
        \item Calcola $f'(x_k)$ e $f''(x)$
        \item Si pone $x_{k+1} = x_k -  \frac{f'(x_k)}{f''(x_k)}$
    \end{itemize} 
\end{itemize}
Criterio di arresto: 